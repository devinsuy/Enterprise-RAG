{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip -q install git+https://github.com/huggingface/transformers\n",
    "!pip install -q datasets loralib sentencepiece\n",
    "!pip -q install bitsandbytes accelerate\n",
    "!pip -q install langchain\n",
    "!pip install einops\n",
    "!pip install faiss-gpu\n",
    "!pip install --upgrade --quiet  langchain-community chromadb bs4 qdrant-client\n",
    "!pip install langchainhub\n",
    "\n",
    "!pip install --upgrade --quiet  wikipedia\n",
    "!pip install --upgrade --quiet  arxiv\n",
    "!pip install --upgrade --quiet  pymupdf\n",
    "\n",
    "!pip install xmltodict\n",
    "\n",
    "!pip install cohere\n",
    "!pip install -U langchain-cohere\n",
    "!pip install evaluate\n",
    "!pip install bert_score\n",
    "\n",
    "!pip install --upgrade tensorflow\n",
    "!pip install -U accelerate\n",
    "!pip install sentence_transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import bs4\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "import locale\n",
    "\n",
    "\n",
    "from pprint import pprint\n",
    "from transformers import AutoTokenizer , AutoModelForCausalLM\n",
    "from transformers import pipeline, BitsAndBytesConfig\n",
    "\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.utils.math import cosine_similarity\n",
    "\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "from langchain_community.document_loaders import OnlinePDFLoader\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.document_loaders import PubMedLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locale.getpreferredencoding = lambda: \"UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LLM_pipeline(llm_name = \"microsoft/Phi-3-mini-4k-instruct\", temperature = 0.5, top_p =0.95, max_length = 4096):\n",
    "    quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                         llm_int4_enable_fp32_cpu_offload=True)\n",
    "    llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "        llm_name,\n",
    "        torch_dtype=torch.float32,\n",
    "        device_map='auto',\n",
    "        quantization_config=quantization_config, \n",
    "        trust_remote_code = True\n",
    "    )\n",
    "\n",
    "    llm_tokenizer = AutoTokenizer.from_pretrained(llm_name)\n",
    "    llm_pipeline = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=llm_model,\n",
    "        tokenizer=llm_tokenizer,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.2\n",
    "    )\n",
    "\n",
    "    ##TODO## \n",
    "    # Add EOS_token\n",
    "    return llm_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_spliiter(Docs, Chunk_size = 20, Overlap = 200):\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size= Chunk_size, chunk_overlap = Overlap)\n",
    "    splits = text_spliiter.split_documents(Docs)\n",
    "    for idx, text in enumerate(splits):\n",
    "      splits[idx].metadata['split_id'] = idx\n",
    "    \n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PDFloader(directory_path):\n",
    "    pdf_data = []\n",
    "    # Iterate through all files in the directory\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            loader = PyMubPDFLoader(file_path)\n",
    "            data = loader.load()\n",
    "            pdf_data.append(data[0])\n",
    "    return pdf_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HTMLLoader(directory_path):\n",
    "    return none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VectorDB(Splits, Embeddings, Collection_name, TOP_K):\n",
    "    qdrant_vectorstore = Qdrant.from_documents(splits = Splits,\n",
    "    Embeddings,\n",
    "    location=\":memory:\",  # Local mode with in-memory storage only\n",
    "    collection_name= Collection_name,\n",
    "    force_recreate=True\n",
    "    )\n",
    "    retriever = qdrant_vectorstore.as_retriever(search_kwargs={\"k\": TOP_K})\n",
    "\n",
    "    return retriever, qdrant_vectorstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Add_documents_to_VectorDB(Splits, Qdrant_vectorstore):\n",
    "    Qdrant_vectorstore.add_documents(documents = Splits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
