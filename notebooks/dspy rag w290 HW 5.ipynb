{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Chain of thought of RAG Pipeline using Ollama x llama2, Qdrant and DSPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install dspy-ai\n",
    "# !pip install qdrant-client\n",
    "# !pip install ollama\n",
    "# !pip install pypdf langchain_community\n",
    "# !pip install cryptography \n",
    "# !pip install \"dspy-ai[qdrant]\"\n",
    "# !pip install --upgrade transformers\n",
    "# !pip install --upgrade sentence_transformers\n",
    "#!pip install protobuf==3.20\n",
    "# !pip install --upgrade --quiet  wikipedia\n",
    "# !pip install --upgrade --quiet  arxiv\n",
    "# !pip install --upgrade --quiet  pymupdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import bs4\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "from pprint import pprint\n",
    "import dspy\n",
    "import locale\n",
    "\n",
    "# from transformers import AutoTokenizer , AutoModelForCausalLM\n",
    "# from transformers import pipeline, BitsAndBytesConfig\n",
    "\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.utils.math import cosine_similarity\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "from langchain_community.document_loaders import OnlinePDFLoader\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.document_loaders import PubMedLoader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import models, QdrantClient\n",
    "from dspy.retrieve.qdrant_rm import QdrantRM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_embeddings = HuggingFaceEmbeddings(model_name=\"multi-qa-mpnet-base-dot-v1\")\n",
    "# encoder = SentenceTransformer('multi-qa-mpnet-base-dot-v1') # Model to create embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset into Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Function to characterize chunk_size, overlap and top_k impact to the RAG system\n",
    "\n",
    "# def vector_DB(base_embeddings, CHUNK_SIZE, OVERLAP, TOP_K):\n",
    "\n",
    "#   text_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap= int(OVERLAP*CHUNK_SIZE))\n",
    "#   #assign a unique number to each document we ingest\n",
    "#   global_doc_number = 1\n",
    "#   arxiv_numbers = ('2005.11401', '2104.07567', '2104.09864', '2105.03011', '2106.09685', '2203.02155', '2211.09260', '2211.12561',\n",
    "#                  '2212.09741', '2305.14314', '2305.18290', '2306.15595', '2309.08872', '2309.15217', '2310.06825', '2310.11511',\n",
    "#                  '2311.08377', '2312.05708', '2401.06532', '2402.01306')\n",
    "#   all_arxiv_pages = []\n",
    "\n",
    "#   #loop through the papers\n",
    "#   for identifier in arxiv_numbers:\n",
    "#     # Construct URL using the arXiv unique identifier\n",
    "#     arx_url = f\"https://arxiv.org/pdf/{identifier}.pdf\"\n",
    "\n",
    "#     # Extract pages from the document and add them to the list of pages\n",
    "#     arx_loader = PyMuPDFLoader(arx_url)\n",
    "#     arx_pages = arx_loader.load()\n",
    "#     for page_num in range(len(arx_pages)):\n",
    "#       page = arx_pages[page_num]\n",
    "#         #CHANGED\n",
    "#       page.metadata['page_num'] = page_num\n",
    "#       page.metadata['doc_num'] = global_doc_number\n",
    "#       page.metadata['doc_source'] = \"ArXiv\"\n",
    "#       all_arxiv_pages.append(page)\n",
    "\n",
    "#     global_doc_number += 1\n",
    "\n",
    "#   num_pages = len(all_arxiv_pages)\n",
    "#   num_docs = global_doc_number - 1\n",
    "\n",
    "#   #print(f\"{num_docs} documents in total\")\n",
    "#   #print(f\"{num_pages} pages in total\")\n",
    "\n",
    "#   #index doc chunks\n",
    "#   splits = text_splitter.split_documents(all_arxiv_pages)\n",
    "#   for idx, text in enumerate(splits):\n",
    "#       splits[idx].metadata['split_id'] = idx\n",
    "\n",
    "#   #print('Number of splits/chunks: ', len(splits))\n",
    "\n",
    "#   print(type(splits))\n",
    "#   print(splits)\n",
    "#   qdrant_vectorstore = Qdrant.from_documents(splits,\n",
    "#     base_embeddings,\n",
    "#     location=\":memory:\",  # Local mode with in-memory storage only\n",
    "#     collection_name=\"rag_tech_db\",\n",
    "#     force_recreate=True\n",
    "#   )\n",
    "\n",
    "#   retriever = qdrant_vectorstore.as_retriever(search_kwargs={\"k\": TOP_K})\n",
    "\n",
    "#   # qdrant_vectorstore.add_documents(documents=splits)\n",
    "\n",
    "\n",
    "#   wiki_docs = WikipediaLoader(query=\"Generative Artificial Intelligence\", load_max_docs=4).load()\n",
    "#   for idx, text in enumerate(wiki_docs):\n",
    "#     wiki_docs[idx].metadata['doc_num'] = global_doc_number\n",
    "#     wiki_docs[idx].metadata['doc_source'] = \"Wikipedia\"\n",
    "\n",
    "#   global_doc_number += 1\n",
    "\n",
    "#   # print('Number of documents: ', len(wiki_docs))\n",
    "\n",
    "#   #index docs\n",
    "#   wiki_splits = text_splitter.split_documents(wiki_docs)\n",
    "#   for idx, text in enumerate(wiki_splits):\n",
    "#     wiki_splits[idx].metadata['split_id'] = idx\n",
    "\n",
    "#   # print('Number of splits/chunks: ', len(wiki_splits))\n",
    "\n",
    "#   qdrant_vectorstore.add_documents(documents=wiki_splits)\n",
    "\n",
    "#   wiki_docs = WikipediaLoader(query=\"Information Retrieval\", load_max_docs=4).load()\n",
    "#   for idx, text in enumerate(wiki_docs):\n",
    "#     wiki_docs[idx].metadata['doc_num'] = global_doc_number\n",
    "#     wiki_docs[idx].metadata['doc_source'] = \"Wikipedia\"\n",
    "\n",
    "#   global_doc_number += 1\n",
    "\n",
    "#   # print('Number of documents: ', len(wiki_docs))\n",
    "\n",
    "#   #index docs\n",
    "#   wiki_splits = text_splitter.split_documents(wiki_docs)\n",
    "#   for idx, text in enumerate(wiki_splits):\n",
    "#     wiki_splits[idx].metadata['split_id'] = idx\n",
    "\n",
    "#   # print('Number of splits/chunks: ', len(wiki_splits))\n",
    "#   qdrant_vectorstore.add_documents(documents=wiki_splits)\n",
    "\n",
    "#   wiki_docs = WikipediaLoader(query=\"Large Language Models\", load_max_docs=4).load()\n",
    "#   for idx, text in enumerate(wiki_docs):\n",
    "#     wiki_docs[idx].metadata['doc_num'] = global_doc_number\n",
    "#     wiki_docs[idx].metadata['doc_source'] = \"Wikipedia\"\n",
    "\n",
    "#   global_doc_number += 1\n",
    "\n",
    "#   # print('Number of documents: ', len(wiki_docs))\n",
    "\n",
    "#   #index docs\n",
    "#   wiki_splits = text_splitter.split_documents(wiki_docs)\n",
    "#   for idx, text in enumerate(wiki_splits):\n",
    "#     wiki_splits[idx].metadata['split_id'] = idx\n",
    "\n",
    "#   # print('Number of splits/chunks: ', len(wiki_splits))\n",
    "\n",
    "#   qdrant_vectorstore.add_documents(documents=wiki_splits)\n",
    "\n",
    "#   web_loader = WebBaseLoader(\n",
    "#     web_paths=(\"https://lilianweng.github.io/posts/2020-10-29-odqa/\",\n",
    "#                \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "#                \"https://lilianweng.github.io/posts/2018-06-24-attention/\"),\n",
    "\n",
    "#     bs_kwargs=dict(\n",
    "#         parse_only=bs4.SoupStrainer(\n",
    "#             class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "#         )\n",
    "#     ),\n",
    "#   )\n",
    "\n",
    "#   web_documents = web_loader.load()\n",
    "\n",
    "#   for idx, text in enumerate(web_documents):\n",
    "#     web_documents[idx].metadata['doc_num'] = global_doc_number\n",
    "#     web_documents[idx].metadata['doc_source'] = \"WWW\"\n",
    "#   global_doc_number += 1\n",
    "\n",
    "#   # print('Number of documents: ', len(web_documents))\n",
    "\n",
    "#   web_splits = text_splitter.split_documents(web_documents)\n",
    "\n",
    "#   for idx, text in enumerate(web_splits):\n",
    "#     web_splits[idx].metadata['split_id'] = idx\n",
    "\n",
    "#   # print('Number of splits: ', len(web_splits))\n",
    "\n",
    "#   qdrant_vectorstore.add_documents(documents=web_splits)\n",
    "\n",
    "#   return retriever, qdrant_vectorstore\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever, qdrant_vectorstore = vector_DB(base_embeddings, 50, 0.1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to characterize chunk_size, overlap and top_k impact to the RAG system\n",
    "\n",
    "\n",
    "def vector_DB_Dspy_test(base_embeddings, CHUNK_SIZE, OVERLAP, TOP_K):\n",
    "\n",
    " \n",
    "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap= int(OVERLAP*CHUNK_SIZE))\n",
    "  #assign a unique number to each document we ingest\n",
    "  global_doc_number = 1\n",
    "  arxiv_numbers = ('2005.11401', '2104.07567', '2104.09864', '2105.03011', '2106.09685', '2203.02155', '2211.09260', '2211.12561',\n",
    "                 '2212.09741', '2305.14314', '2305.18290', '2306.15595', '2309.08872', '2309.15217', '2310.06825', '2310.11511',\n",
    "                 '2311.08377', '2312.05708', '2401.06532', '2402.01306')\n",
    "  all_arxiv_pages = []\n",
    "\n",
    "  #loop through the papers\n",
    "  for identifier in arxiv_numbers:\n",
    "    # Construct URL using the arXiv unique identifier\n",
    "    arx_url = f\"https://arxiv.org/pdf/{identifier}.pdf\"\n",
    "\n",
    "    # Extract pages from the document and add them to the list of pages\n",
    "    arx_loader = PyMuPDFLoader(arx_url)\n",
    "    arx_pages = arx_loader.load()\n",
    "    for page_num in range(len(arx_pages)):\n",
    "      page = arx_pages[page_num]\n",
    "        #CHANGED\n",
    "      page.metadata['page_num'] = page_num\n",
    "      page.metadata['doc_num'] = global_doc_number\n",
    "      page.metadata['doc_source'] = \"ArXiv\"\n",
    "      all_arxiv_pages.append(page)\n",
    "\n",
    "    global_doc_number += 1\n",
    "\n",
    "  num_pages = len(all_arxiv_pages)\n",
    "  num_docs = global_doc_number - 1\n",
    "\n",
    "  #print(f\"{num_docs} documents in total\")\n",
    "  #print(f\"{num_pages} pages in total\")\n",
    "\n",
    "  #index doc chunks\n",
    "  splits = text_splitter.split_documents(all_arxiv_pages)\n",
    "  for idx, text in enumerate(splits):\n",
    "      splits[idx].metadata['split_id'] = idx\n",
    "\n",
    "  \n",
    "\n",
    "  #print('Number of splits/chunks: ', len(splits))\n",
    "\n",
    "  qdrant_client = QdrantClient(\":memory:\")  # loads in memory for this session\n",
    "\n",
    "  # # Create collection to store books\n",
    "  # qdrant_client.create_collection(\n",
    "  #   collection_name=\"rag_tech_db\",\n",
    "  #   vectors_config=models.VectorParams(\n",
    "  #       size=768, # Vector size is defined by used model\n",
    "  #       distance=models.Distance.DOT,\n",
    "  #   ),\n",
    "  # )\n",
    "  # # Let's vectorize descriptions and upload to qdrant\n",
    "\n",
    "  doc_contents = [doc.page_content for doc in splits]\n",
    "  doc_ids = list(range(1, len(splits) + 1))\n",
    "  metadata = [doc.metadata for doc in splits]\n",
    "    \n",
    "  qdrant_client.add(collection_name=\"rag_tech_db\",\n",
    "    documents=doc_contents,\n",
    "    metadata=metadata,\n",
    "    ids=doc_ids\n",
    "  )\n",
    "\n",
    "  # qdrant_client.upload_records(\n",
    "  #   collection_name=\"rag_tech_db\",\n",
    "  #   records=[\n",
    "  #       models.Record(\n",
    "  #           id=idx,\n",
    "  #           vector=encoder.encode(doc.page_content).tolist(),\n",
    "  #           payload=doc.metadata\n",
    "  #       ) for idx, doc in enumerate(splits)\n",
    "  #   ]\n",
    "  # )\n",
    "\n",
    "\n",
    "  qdrant_retriever_model = QdrantRM(\"rag_tech_db\", qdrant_client, k=TOP_K)\n",
    "\n",
    "\n",
    "  return qdrant_retriever_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_embeddings = 10\n",
    "qdrant_retreiver_model = vector_DB_Dspy_test(base_embeddings, 800, 0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Ollama model\n",
    "\n",
    "Pre-requisites: should have downlaoded the Ollama application from: https://ollama.com/download/windows\n",
    "\n",
    "For Windows you can follow these steps:\n",
    "1) To download it, go to: Download Ollama on Windows. \n",
    "2) Install it on your system.\n",
    "3) After installing, you can open the command prompt and type “ollama pull llama2”, which will download the latest quantized image for Llama2; by default, it pulls a 7B model.\n",
    "4) You will see the Ollama icon in your hidden icons, which means that you can run Ollama models now!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_model = dspy.OllamaLocal(model=\"phi3:medium\",model_type='text',\n",
    "                                max_tokens=4096,\n",
    "                                temperature=0.7,\n",
    "                                top_p=0.9, frequency_penalty=1.17, top_k=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try out one example before proceeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' \"Interstellar\" is a science fiction film directed by Christopher Nolan, released in 2014. The story revolves around a group of astronauts who embark on an impossible mission through a wormhole near Saturn to find humanity’s new home because Earth has become uninhabitable due to crop blights and global dust storms.\\n\\nThe protagonist, Cooper (played by Matthew McConaughey), is a former NASA pilot turned farmer who finds himself recruited for the mission after his daughter Murphy discovers an anomaly in gravitational theory that could lead them through space-time using gravity as a navigable dimension. The film\\'s plot centers on complex concepts of theoretical physics, including time dilation and black holes, to explore deep space travel possibilities.\\n\\nCooper and the team aboard the Endurance spaceship face various challenges in their quest for survival while trying to solve an equation that could help them find a new home before humanity\\'s resources on Earth run out. The film also explores profound emotional relationships, including Cooper’s connection with his daughter and colleagues as they confront the ultimate sacrifice required by their mission.\\n\\n\"Interstellar\" received critical acclaim for its visual effects, storytelling techniques that involve complex scientific theories like relativity, and themes of love transcending time and space. The film also raises existential questions about human destiny in a universe where survival often requires leaving everything behind to adapt and evolve.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama_model(\"tell me about interstellar's plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Top Passages from the retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 passages for question: When was the transformer architecture introduced, and by which organization? \n",
      " ------------------------------ \n",
      "\n",
      "1] In this work, we proposed a new position embedding method that incorporates explicit relative position dependency in\n",
      "self-attention to enhance the performance of transformer architectures. Our theoretical analysis indicates that relative\n",
      "position can be naturally formulated using vector production in self-attention, with absolution position information\n",
      "being encoded through a rotation matrix. In addition, we mathematically illustrated the advantageous properties of\n",
      "the proposed method when applied to the Transformer. Finally, experiments on both English and Chinese benchmark\n",
      "datasets demonstrate that our method encourages faster convergence in pre-training. The experimental results also show\n",
      "that our proposed RoFormer can achieve better performance on long texts task.\n",
      "References \n",
      "\n",
      "2] datasets show that the enhanced transformer with rotary position embedding, namely RoFormer, can give better\n",
      "performance compared to baseline alternatives and thus demonstrates the efficacy of the proposed RoPE.\n",
      "In brief, our contributions are three-folds as follows:\n",
      "• We investigated the existing approaches to the relative position encoding and found that they are mostly\n",
      "built based on the idea of the decomposition of adding position encoding to the context representations. We\n",
      "introduce a novel method, namely Rotary Position Embedding(RoPE), to leverage the positional information\n",
      "into the learning process of PLMS. The key idea is to encode relative position by multiplying the context\n",
      "representations with a rotation matrix with a clear theoretical interpretation. \n",
      "\n",
      "3] ROFORMER: ENHANCED TRANSFORMER WITH ROTARY\n",
      "POSITION EMBEDDING\n",
      "Jianlin Su\n",
      "Zhuiyi Technology Co., Ltd.\n",
      "Shenzhen\n",
      "bojonesu@wezhuiyi.com\n",
      "Yu Lu\n",
      "Zhuiyi Technology Co., Ltd.\n",
      "Shenzhen\n",
      "julianlu@wezhuiyi.com\n",
      "Shengfeng Pan\n",
      "Zhuiyi Technology Co., Ltd.\n",
      "Shenzhen\n",
      "nickpan@wezhuiyi.com\n",
      "Ahmed Murtadha\n",
      "Zhuiyi Technology Co., Ltd.\n",
      "Shenzhen\n",
      "mengjiayi@wezhuiyi.com\n",
      "Bo Wen\n",
      "Zhuiyi Technology Co., Ltd.\n",
      "Shenzhen\n",
      "brucewen@wezhuiyi.com\n",
      "Yunfeng Liu\n",
      "Zhuiyi Technology Co., Ltd.\n",
      "Shenzhen\n",
      "glenliu@wezhuiyi.com\n",
      "November 9, 2023\n",
      "ABSTRACT\n",
      "Position encoding recently has shown effective in the transformer architecture. It enables valuable\n",
      "supervision for dependency modeling between elements at different positions of the sequence. In \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dspy.settings.configure(rm=qdrant_retreiver_model, lm=ollama_model)\n",
    "\n",
    "dev_example = \"When was the transformer architecture introduced, and by which organization?\"\n",
    "\n",
    "def get_top_passages(question):\n",
    "    retrieve = dspy.Retrieve(k=3)\n",
    "    topK_passages = retrieve(question,k=3).passages\n",
    "    print(f\"Top {retrieve.k} passages for question: {question} \\n\", '-' * 30, '\\n')\n",
    "    for idx, passage in enumerate(topK_passages):\n",
    "        print(f'{idx+1}]', passage, '\\n')\n",
    "\n",
    "get_top_passages(dev_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Signatures for Input and Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateAnswer(dspy.Signature):\n",
    "    \"\"\"Answer questions with short factoid answers.\"\"\"\n",
    "\n",
    "    context = dspy.InputField(desc=\"may or maynot contain relevant facts or answer keywords\")\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"an answer between 10 to 20 words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ga = GenerateAnswer(context=\"My name is sachin and I like writing blogs\", question=\"What is my name?\", answer=\"Sachin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method BaseModel.model_construct of GenerateAnswer(context, question -> answer\n",
      "    instructions='Answer questions with short factoid answers.'\n",
      "    context = Field(annotation=str required=True json_schema_extra={'desc': 'may or maynot contain relevant facts or answer keywords', '__dspy_field_type': 'input', 'prefix': 'Context:'})\n",
      "    question = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Question:', 'desc': '${question}'})\n",
      "    answer = Field(annotation=str required=True json_schema_extra={'desc': 'an answer between 10 to 20 words', '__dspy_field_type': 'output', 'prefix': 'Answer:'})\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "print(ga.model_construct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a DSPy CoT Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG(dspy.Module):\n",
    "    def __init__(self, num_passages=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.retrieve = dspy.Retrieve(k=num_passages)\n",
    "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
    "\n",
    "    def forward(self, question):\n",
    "        context = self.retrieve(question).passages\n",
    "        prediction = self.generate_answer(context=context, question=question)\n",
    "        return dspy.Prediction(context=context, answer=prediction.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call the RAG object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncompiled_rag = RAG()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_questions_answers = {\n",
    "    0: {\"question\": \"What purpose do large language models serve in the field of natural language processing?\",\n",
    "  \"gold_answer_research\": \"Large language models (LLMs) serve the purpose of enabling general-purpose language generation and other natural language processing tasks such as classification. They achieve this by learning statistical relationships from text documents during computationally intensive self-supervised and semi-supervised training. LLMs can be used for text generation by predicting the next token or word, making them valuable for tasks like speech recognition, machine translation, and information retrieval. Additionally, LLMs have superseded previous models like recurrent neural networks, showcasing their efficiency and effectiveness in NLP tasks.\",\n",
    "  \"gold_answer_marketing\": \"Large language models serve the purpose of improving performance in various natural language processing tasks, such as speech recognition, machine translation, natural language generation, optical character recognition, handwriting recognition, grammar induction, and information retrieval.\"},\n",
    "1: {\"question\": \"How does a large language model learn from text during training?\",\n",
    "  \"gold_answer_research\": \"A large language model learns from text during training by first going through an unsupervised generative 'pretraining' stage where it sets initial parameters using a language modeling objective. Then, it goes through a supervised discriminative 'fine-tuning' stage where it refines its parameters based on annotated examples or task demonstrations. This dual-stage approach allows the model to learn statistical relationships from text documents in a computationally intensive process, enabling it to achieve general-purpose language generation and natural language processing tasks.\",\n",
    "  \"gold_answer_marketing\": \"A large language model learns from text during training by first pretraining on a diverse dataset to acquire general language knowledge, and then fine-tuning on specific tasks or demonstrations to adapt its parameters for more targeted performance.\"},\n",
    "2: {\"question\": \"What are some key architectures behind the development of large language models?\",\n",
    "  \"gold_answer_research\": \"Key architectures behind the development of large language models include the use of self-attention mechanisms, such as those seen in Transformer decoders. These architectures have been applied to tasks like autoregressive language modeling and have led to the dominance of Transformer-based language models in NLP. Models like BERT and GPT-2 have further advanced this paradigm, showcasing the power of large Transformer language models in achieving state-of-the-art results across various NLP tasks. Additionally, architectures like neural-retriever-in-the-loop generative-based models have shown improvements in tasks like open-domain QA and knowledge-grounded dialogue, emphasizing the importance of consistent and engaging responses in long-form generation and multi-turn conversations.\",\n",
    "  \"gold_answer_marketing\": \"Key architectures behind the development of large language models include Transformer-based models such as BERT and GPT-2, which utilize self-attention mechanisms for tasks like autoregressive language modeling and knowledge-grounded dialogue. These models have shown significant success in NLP tasks and have led to advancements in general-purpose language generation and natural language processing.\"},\n",
    "3: {\"question\": \"Can you name some specific large language models and the companies or organizations that have developed them?\",\n",
    "  \"gold_answer_research\": \"Some specific large language models include GPT-3 by OpenAI, Chinchilla by DeepMind, and BERT by Google. OpenAI developed GPT-3, DeepMind developed Chinchilla, and Google developed BERT. These models have been significant advancements in the field of natural language processing.\",\n",
    "  \"gold_answer_marketing\": \"Chinchilla by DeepMind, GPT-3 by OpenAI.\"},\n",
    "7: {\"question\": \"What licensing models have been adopted for the distribution of source-available language models?\",\n",
    "  \"gold_answer_research\": \"Based on the provided context, it seems that licensing models for the distribution of source-available language models have not been explicitly discussed in the referenced papers. However, it is crucial to consider potential licensing options such as open-source licenses (e.g., GPL, MIT) or proprietary licenses when distributing language models to ensure legal compliance and control over usage rights. Additionally, considering the implications of different licensing models on accessibility, collaboration, and commercialization is essential for determining the most suitable approach for sharing language models with the community. Further research or consultation with legal experts may be necessary to explore specific licensing strategies for source-available language models.\",\n",
    "  \"gold_answer_marketing\": \"Answer: Some organizations choose open-sourcing, while others restrict access to a few organizations with resources or offer end-to-end deployment via API.\"},\n",
    "8: {\"question\": \"What are language models and what is their purpose in natural language processing?\",\n",
    "  \"gold_answer_research\": \"Language models are probabilistic models of natural language that help predict or correct text. Their purpose in natural language processing is to assist in various tasks such as speech recognition, machine translation, natural language generation, and information retrieval. By analyzing the performance of human subjects, language models improve the understanding and generation of human-like text.\",\n",
    "  \"gold_answer_marketing\": \"Language models are probabilistic models of natural language that are used in tasks such as speech recognition, machine translation, and natural language generation in natural language processing.\"},\n",
    "9: {\"question\": \"How have language models evolved in terms of architecture, from the 1980s to present times?\",\n",
    "  \"gold_answer_research\": \"Language models have evolved significantly in terms of architecture from the 1980s to present times. In the 1980s, the first statistical language model was proposed, leading to experiments by IBM that identified areas for improvement by observing human subjects. However, it wasn't until 2017 when the transformer architecture was introduced by Google, revolutionizing the field. This development paved the way for models like BERT in 2018, which marked a shift towards large-scale transformer-based language models. These modern architectures, based on self-attention mechanisms, have dominated the field of natural language processing, achieving state-of-the-art performance in various tasks.\",\n",
    "  \"gold_answer_marketing\": \"Language models have evolved from early statistical models in the 1980s to modern transformer architectures, such as BERT and GPT-2, which use self-attention mechanisms and have become dominant in natural language processing tasks.\"},\n",
    "11: {\"question\": \"Can you explain how maximum entropy language models work and what the partition function signifies?\",\n",
    "  \"gold_answer_research\": \"Maximum entropy language models use feature functions to encode the relationship between a word and its n-gram history, aiming to maximize reward while satisfying a KL-constrained objective. The partition function, denoted as Z(x), is crucial in normalizing the probabilities of all possible outputs given the input. It represents the sum of the exponential of the reward function over all possible output sequences, making it computationally expensive to estimate but essential for accurate modeling. The partition function ensures that the model's predicted probabilities sum up to 1, providing a foundation for effective language modeling.\",\n",
    "  \"gold_answer_marketing\": \"Maximum entropy language models encode the relationship between a word and the n-gram history using feature functions. The partition function in this context represents the total probability of all possible outcomes, making it a crucial factor in determining the optimal solution for the reward maximization objective.\"},\n",
    "12: {\"question\": \"What is the benefit of using continuous space embeddings in recurrent neural network language models?\",\n",
    "  \"gold_answer_research\": \"Continuous space embeddings in recurrent neural network language models help alleviate the curse of dimensionality by representing words as non-linear combinations of weights in the embedding space. This approach helps address the data sparsity problem caused by the exponential increase in possible word sequences with vocabulary size. By utilizing continuous space embeddings, neural networks can effectively capture semantic relationships and meaning within the language model.\",\n",
    "  \"gold_answer_marketing\": \"Continuous space embeddings in recurrent neural network language models help alleviate the curse of dimensionality caused by the exponential increase in possible word sequences, reducing data sparsity issues.\"},\n",
    "13: {\"question\": \"What challenges do large language models face in mirroring human cognitive patterns?\",\n",
    "  \"gold_answer_research\": \"Large language models face challenges in mirroring human cognitive patterns because they sometimes learn patterns that humans do not learn, while also failing to learn patterns that humans typically learn. This discrepancy suggests that the models may not be plausible cognitive models, despite matching human performance in some tasks. Further research is needed to address these limitations and improve the alignment of large language models with human cognitive patterns.\",\n",
    "  \"gold_answer_marketing\": \"Large language models sometimes learn patterns that humans do not learn and fail to learn patterns that humans typically do learn.\"},\n",
    "16: {\"question\": \"What factors influenced the development of generative language models by Anthropic?\",\n",
    "  \"gold_answer_research\": \"Several factors influenced the development of generative language models by Anthropic, including the limitations in coding, math, and reasoning capabilities of the initial version Claude, the partnerships with companies like Notion and Quora to enhance the model's capabilities, and the need to address biases, unsafe content, and ethical considerations in training data. Additionally, the reliance on supervised learning and the need for controlled generation in generative models played a role in shaping the development of Anthropic's language models.\",\n",
    "  \"gold_answer_marketing\": \"Factors that influenced the development of generative language models by Anthropic include partnerships with companies like Notion and Quora, limitations in coding, math, and reasoning capabilities in initial models like Claude, and the need to address biases and unsafe content in training datasets.\"},\n",
    "17: {\"question\": \"What is Constitutional AI and how does it affect the functionality of AI systems?\",\n",
    "  \"gold_answer_research\": \"Constitutional AI is an approach developed by Anthropic for training AI systems, particularly language models like Claude, to be harmless and helpful without relying on extensive human feedback. It involves two phases: supervised learning, where the model generates responses to prompts and self-critiques based on a set of guiding principles, and reinforcement learning, where the model is trained with AI-generated feedback according to constitutional principles. This approach enables the training of AI assistants that are both helpful and harmless, with the ability to explain objections to harmful requests, enhancing transparency and reducing the need for human supervision.\",\n",
    "  \"gold_answer_marketing\": \"Constitutional AI is an approach developed by Anthropic for training AI systems, particularly language models like Claude, to be harmless and helpful without relying on extensive human feedback. It involves supervised learning and reinforcement learning phases to guide the model's responses based on a set of guiding principles (a 'constitution'). This approach aims to create AI systems that are both helpful and transparent in their decision-making process, reducing the need for constant human supervision.\"},\n",
    "18: {\"question\": \"How do advances in AI models impact their ability to interact with different types of data, such as images?\",\n",
    "  \"gold_answer_research\": \"Advances in AI models, such as multimodal models like RA-CM3, have significantly improved their ability to interact with different types of data, such as images. These models can refer to external memory, like web data, to increase their knowledge capacity, allowing them to generate correct images from entity-rich captions. Additionally, these models can perform image editing and manually specify examples in-context for better results. The use of large language models, combined with larger datasets and neural networks, has also enhanced their performance in tasks like image generation and text generation.\",\n",
    "  \"gold_answer_marketing\": \"Advances in AI models, such as multimodal models like RA-CM3, allow for better interaction with different types of data, like images, by accessing external memory for increased knowledge capacity and improving performance in tasks like image generation and image editing.\"},\n",
    "19: {\"question\": \"What are the potential trade-offs between AI system alignment with ethical guidelines and practical utility?\",\n",
    "  \"gold_answer_research\": \"The potential trade-offs between AI system alignment with ethical guidelines and practical utility include the risk of reduced performance and usability due to stringent ethical alignment measures, as seen with Claude 2. Users may face limitations and refusal of assistance for benign requests, leading to debates over the 'alignment tax' in AI development. Balancing ethical considerations with practical functionality is crucial to ensure alignment with ethical guidelines without compromising the practical utility of AI systems. Research is needed to find a middle ground that prioritizes ethical alignment while maintaining usability and performance.\",\n",
    "  \"gold_answer_marketing\": \"The potential trade-offs between AI system alignment with ethical guidelines and practical utility include balancing stringent ethical alignment that may reduce usability and performance, ensuring transparency and fairness in alignment processes, and addressing the alignment tax that may impact adoption of AI systems.\"},\n",
    "20: {\"question\": \"How has the token handling capacity changed between different versions of the Claude model?\",\n",
    "  \"gold_answer_research\": \"The token handling capacity has increased with each new version of the Claude model. Claude Instant has a context length of 100,000 tokens, Claude 2.1 doubled this to 200,000 tokens, and Claude 3 Opus default version has a context window of 200,000 tokens but can be expanded to 1 million for specific use cases. This progression shows a trend towards handling larger amounts of text data for improved performance and capabilities.\",\n",
    "  \"gold_answer_marketing\": \"The token handling capacity has increased from Claude to Claude Instant to Claude 2.1, with Claude Instant having a input context length of 100,000 tokens, Claude 2.1 having a context window of 200,000 tokens, and Claude 3 Opus having a context window of 1 million tokens.\"},\n",
    "22: {\"question\": \"In what ways has the Claude model's ability to self-critique and revise its responses enhanced its transparency?\",\n",
    "  \"gold_answer_research\": \"The Claude model's ability to self-critique and revise its responses has enhanced its transparency by allowing for iterative improvements based on past actions and mistakes. Through self-reflection, the model can refine its output by learning from feedback and generating special tokens to signal the need for retrieval or confirm the relevance, support, or completeness of its responses. This process ensures that the model's statements about the world are truthful and accurate, ultimately increasing transparency in its decision-making and reasoning processes.\",\n",
    "  \"gold_answer_marketing\": \"The Claude model's ability to self-critique and revise its responses has enhanced its transparency by allowing it to generate text informed by retrieved passages, criticize the output, and signal the need for retrieval or confirm the output's relevance, support, or completeness. This self-reflection process helps improve the model's accuracy and reliability in generating responses.\"},\n",
    "23: {\"question\": \"How do subsequent versions of Claude compare in terms of their likelihood to produce false statements?\",\n",
    "  \"gold_answer_research\": \"Claude Instant is a faster and lighter version of Claude, with an input context length of 100,000 tokens. In contrast, Claude 3 has faced criticism for its stringent ethical alignment, leading to a debate over the 'alignment tax' in AI development. Users have been refused assistance with benign requests, which has sparked discussions on balancing ethical considerations and practical functionality. This suggests that Claude Instant may have a lower likelihood of producing false statements compared to Claude 3 due to its focus on usability and performance.\",\n",
    "  \"gold_answer_marketing\": \"Claude Instant is a faster, less expensive, and lighter version of Claude with a shorter input context length. Claude 3 has faced criticism for ethical alignment issues that may affect usability and performance.\"},\n",
    "24: {\"question\": \"Who developed the language model family known as Chinchilla?\",\n",
    "  \"gold_answer_research\": \"The Chinchilla language model family was developed by the research team at DeepMind and presented in March 2022. It is named 'Chinchilla' as an advancement over the previous Gopher model family. The Chinchilla family has been trained to investigate the scaling laws of large language models and is designed to outperform GPT-3.\",\n",
    "  \"gold_answer_marketing\": \"The research team at DeepMind developed the language model family known as Chinchilla.\"},\n",
    "25: {\"question\": \"What benchmark did Chinchilla achieve an average accuracy of 67.5% on?\",\n",
    "  \"gold_answer_research\": \"Chinchilla achieved an average accuracy of 67.5% on the MMLU benchmark (Measuring Massive Multitask Language Understanding).\",\n",
    "  \"gold_answer_marketing\": \"Chinchilla achieved an average accuracy of 67.5% on the MMLU benchmark (Measuring Massive Multitask Language Understanding).\"},\n",
    "27: {\"question\": \"What is the relationship between Chinchilla and the Gopher language model families?\",\n",
    "  \"gold_answer_research\": \"The Chinchilla family of transformer models is essentially the same as the Gopher family, with minor modifications and different training optimizers. Chinchilla uses AdamW optimizer while Gopher uses Adam optimizer. Additionally, Chinchilla uses relative positional encoding and RMSNorm instead of absolute positional encoding and LayerNorm used by Gopher. Chinchilla has 70B parameters and outperforms Gopher on the MMLU benchmark by 7%, showcasing an improvement in performance. Both families follow similar naming conventions and were developed to investigate the scaling laws of large language models.\",\n",
    "  \"gold_answer_marketing\": \"Chinchilla is a family of transformer models developed by DeepMind, which is a further development over a previous model family named Gopher. Both model families were trained to investigate the scaling laws of large language models.\"},\n",
    "28: {\"question\": \"What distinguishes the architectures of the Chinchilla and Gopher family models in terms of optimization techniques used?\",\n",
    "  \"gold_answer_research\": \"The main distinction in optimization techniques between the Chinchilla and Gopher family models lies in the choice of optimizers. The Gopher family utilizes the Adam optimizer, whereas the Chinchilla family is trained using the AdamW optimizer. Additionally, the Gopher family employs RMSNorm instead of LayerNorm, and relative positional encoding rather than absolute positional encoding. These differences in optimization techniques contribute to the unique characteristics and performance of each model family.\",\n",
    "  \"gold_answer_marketing\": \"The Chinchilla family uses AdamW optimizer, while the Gopher family uses the Adam optimizer.\"},\n",
    "30: {\"question\": \"What is the recommended strategy for training large autoregressive language models with limited compute resources, as contributed by the Chinchilla team?\",\n",
    "  \"gold_answer_research\": \"The Chinchilla team recommends that the number of training tokens should be doubled for every model size doubling to achieve better results on downstream tasks. They also suggest using larger, higher-quality training datasets to improve performance. Additionally, they mention the importance of balancing model size and efficiency to address computational costs and inference latency limitations. It is advised to focus on Transformer language models and consider sharing model parameters for quick task-switching when deploying as a service.\",\n",
    "  \"gold_answer_marketing\": \"The Chinchilla team recommends doubling the number of training tokens for every model size doubling and using larger, higher-quality training datasets to achieve better results on downstream tasks.\"},\n",
    "33: {\"question\": \"What are some key areas of research in the field of artificial intelligence as reflected in recent academic literature?\",\n",
    "  \"gold_answer_research\": \"Recent academic literature in the field of artificial intelligence reflects key areas of research such as natural language processing with state-of-the-art transformers, feature learning in infinite-width neural networks, diverse beam search for complex scene description, and the development of generative AI models capable of generating text and images. Additionally, research focuses on human preferences in dueling bandits, the use of few-shot learners in language models, and the exploration of knowledge-grounded neural conversation models. These areas of research highlight the advancements in AI technology and its applications across various domains.\",\n",
    "  \"gold_answer_marketing\": \"Some key areas of research in artificial intelligence include natural language processing, deep neural networks, generative AI, AI safety, AI art, reinforcement learning, and language agents alignment.\"},\n",
    "34: {\"question\": \"What are some of the limitations of traditional position encoding methods in the architecture of pre-trained language models (PLMs), and what novel approach does the paper propose to address these issues?\",\n",
    "  \"gold_answer_research\": \"One limitation of traditional position encoding methods in PLMs is that they may not enable length extrapolation of pre-existing models, leading to the need for substantial pre-training costs. The paper proposes a novel approach called Position Interpolation, which extends existing PLMs without deviating far from existing definitions of position encoding or attention mechanisms. This method allows for much extended context windows for text modeling, leading to significant perplexity gains and improved model performance.\",\n",
    "  \"gold_answer_marketing\": \"Traditional position encoding methods in PLMs have limitations in enabling length extrapolation and adapting to extended context windows. The paper proposes a novel approach called Position Interpolation, which generates strong models that can effectively make use of much extended context windows. This method allows for substantial pre-training cost savings and preserves the quality of the original models, even for small context window tasks.\"},\n",
    "35: {\"question\": \"How does the Rotary Position Embedding (RoPE) approach in Transformers differ from the traditional additive method of position embedding with respect to encoding position information?\",\n",
    "  \"gold_answer_research\": \"The RoPE approach in Transformers differs from the traditional additive method of position embedding by being multiplicative instead of additive. While traditional methods add position encoding to context representations, RoPE incorporates relative position information through rotation matrix product. This means that RoPE naturally includes relative position dependency in the self-attention formulation, without altering terms in the expanded formulation like the additive method does. Additionally, RoPE's properties show that it decays as the relative distance between positions increases, providing a clear theoretical interpretation of how position information is encoded.\",\n",
    "  \"gold_answer_marketing\": \"The RoPE approach in Transformers differs from the traditional additive method of position embedding by incorporating relative position information through rotation matrix product instead of altering terms in the expanded formulation of additive position encoding.\"},\n",
    "36: {\"question\": \"What is the significance of comparing the normalized subspace similarity between ∆Wq, ∆Wv, and random Gaussian matrices when analyzing the adaptation of pre-trained language models?\",\n",
    "  \"gold_answer_research\": \"Comparing the normalized subspace similarity between ∆Wq, ∆Wv, and random Gaussian matrices provides insight into the underlying mechanism for adapting pre-trained language models. It helps determine the intrinsic rank of the adaptation matrix ∆W and sheds light on the connection between ∆W and the original weight matrix W. By analyzing these similarities, we can understand how much of the adaptation is specific to the task at hand and how much is influenced by the pre-trained model. This comparison is crucial for optimizing the adaptation process and maximizing downstream performance in NLP tasks.\",\n",
    "  \"gold_answer_marketing\": \"Comparing the normalized subspace similarity between ∆Wq, ∆Wv, and random Gaussian matrices helps understand the underlying mechanism for adapting pre-trained language models. It reveals the intrinsic rank and common singular value directions learned by different runs, shedding light on the fundamental principles of using pre-trained language models for downstream tasks in NLP.\"},\n",
    "38: {\"question\": \"What issues are associated with the homogeneity of language model training contractors, and how might it affect the behavior of the models?\",\n",
    "  \"gold_answer_research\": \"The issues associated with the homogeneity of language model training contractors include potential biases in the labeling process, lack of diverse perspectives leading to limited coverage of sensitive content, and reduced robustness in model performance across different tasks. This homogeneity can affect the behavior of the models by reinforcing certain biases, increasing the risk of harmful content generation, and limiting the models' ability to generalize effectively. To address these issues, it is important to ensure diversity among labelers, incorporate varied perspectives in training data, and implement measures to enhance model robustness and performance across a range of tasks.\",\n",
    "  \"gold_answer_marketing\": \"The homogeneity of language model training contractors can lead to biased or limited perspectives in the data, which may result in the models producing harmful content, gaming objectives, or lacking sensitivity to diverse viewpoints. This can affect the behavior of the models by reinforcing stereotypes, increasing toxicity, and reducing their ability to accurately represent under-represented groups.\"},\n",
    "39: {\"question\": \"What are common research topics and themes found in recent publications about artificial intelligence and natural language processing?\",\n",
    "  \"gold_answer_research\": \"Recent publications in artificial intelligence and natural language processing have covered topics such as transformer models, feature learning in neural networks, attention mechanisms, multi-task benchmark platforms, semantic search using sentence embeddings, cross-task generalization, and question generation for question answering. Themes commonly explored include machine comprehension of text, reinforcement learning algorithms, sentence embeddings, semantic compositionality, reasoning with language models and knowledge graphs, and the gap between neural text and human text. These publications also delve into deep language understanding, retrieval-augmented transformers, image captioning, and open datasets for image-text pairs.\",\n",
    "  \"gold_answer_marketing\": \"Common research topics and themes in recent publications on artificial intelligence and natural language processing include transformer models, attention mechanisms, semantic search, sentence embeddings, and question answering using language models and knowledge graphs.\"},\n",
    "41: {\"question\": \"Question: When conducting demographic and technical assessments of teams or research subjects, what types of data categories are typically collected and analyzed to ensure a comprehensive understanding of the group's composition and the methods used?\",\n",
    "  \"gold_answer_research\": \"When conducting demographic and technical assessments of teams or research subjects, it is important to collect and analyze data categories such as age, gender, education level, professional background, and expertise in specific areas. By gathering information on these categories, you can ensure a comprehensive understanding of the group's composition and the methods used in your assessments. Additionally, it may be helpful to consider factors like cultural background, language proficiency, and geographical location to capture a more nuanced picture of the group being assessed. This detailed approach to data collection and analysis can provide valuable insights for making informed decisions and recommendations based on the gathered information.\",\n",
    "  \"gold_answer_marketing\": \"Answer: Demographic data such as age, gender, education level, and technical data related to skills and experience are typically collected and analyzed for comprehensive understanding.\"},\n",
    "43: {\"question\": \"What kind of tasks can be performed using the datasets described in the provided text, and what are some common features of these datasets?\",\n",
    "  \"gold_answer_research\": \"The datasets described in the provided text can be used for tasks such as question answering, duplicate question retrieval, entity retrieval, citation prediction, query understanding, document understanding, passage retrieval, text summarization, fact verification, and code search. Common features of these datasets include diverse task categories, comprehensive instructions, a wide range of synthetic user personalities and interaction patterns, and a focus on enhancing comprehension of documents to deliver accurate results. Additionally, the datasets cover a variety of domains such as public health, scientific exams, climate, and general knowledge.\",\n",
    "  \"gold_answer_marketing\": \"The datasets described in the provided text can be used for tasks such as question answering, document summarization, duplicate question retrieval, code search, sentence simplification, dialogue generation, body retrieval, caption generation, fact verification, and more. Some common features of these datasets include diverse input-output pairs, incorporation of various knowledge-intensive datasets, and a focus on generating high-quality synthetic data points.\"},\n",
    "44: {\"question\": \"What conclusions can be drawn about the relationship between input prompt toxicity and output toxicity when using different language models and prompts?\",\n",
    "  \"gold_answer_research\": \"Based on the findings presented in the results section, it can be concluded that the relationship between input prompt toxicity and output toxicity varies depending on the language model used and the specific prompt given. When instructed to produce a safe and respectful output, InstructGPT models generate less toxic outputs compared to GPT-3, but this advantage disappears when the respectful prompt is removed. On the other hand, when explicitly prompted to produce a toxic output, InstructGPT outputs are much more toxic than GPT-3 outputs. Additionally, the toxicity of the model outputs is highly correlated with the toxicity of the input prompt, as shown in Figure 39.\",\n",
    "  \"gold_answer_marketing\": \"The study found that when instructed to produce a safe and respectful output, InstructGPT models generate less toxic outputs compared to GPT-3. However, this advantage disappears when the respectful prompt is removed. Interestingly, when explicitly prompted to produce a toxic output, InstructGPT outputs are much more toxic than GPT-3. This suggests that the toxicity of the output is highly correlated with the toxicity of the input prompt.\"},\n",
    "45: {\"question\": \"What are some challenges in training retrieval systems and how are negative samples used to address them?\",\n",
    "  \"gold_answer_research\": \"Training retrieval systems face challenges such as redundancy in retrieved documents and lack of diversity in retrieval. Negative samples, including randomly sampled negatives, denoised hard negatives, and instruction-unfollowing negatives, are crucial for improving system performance. Carefully designed negative samples help the system effectively learn the task, but they can also lead to performance drops in out-of-domain datasets. Combining random samples and challenging negatives during training is key to building a competitive system for both in-domain and out-of-domain retrieval.\",\n",
    "  \"gold_answer_marketing\": \"Some challenges in training retrieval systems include high cost of annotating datasets for new tasks and improving performance in zero-shot settings. Negative samples, such as denoised hard negative documents and instruction-unfollowing negative documents, are used to train retrieval systems effectively and address performance drops in out-of-domain datasets.\"},\n",
    "46: {\"question\": \"What factors have been found to potentially impact the ability of models to follow instructions, based on the analysis provided?\",\n",
    "  \"gold_answer_research\": \"Based on the analysis provided, factors that have been found to potentially impact the ability of models to follow instructions include the human feedback obtained from contractors, which may be influenced by their beliefs, cultural backgrounds, and personal history. Additionally, the model's behavior can be affected by false premises in instructions, tendencies to hedge, and performance degradation with multiple explicit constraints in instructions. The models are also not fully aligned or safe, as they can generate toxic or biased outputs, make up facts, and fail to generate reasonable outputs in some cases.\",\n",
    "  \"gold_answer_marketing\": \"Factors that may impact the ability of models to follow instructions include false premises in instructions, models hedging unnecessarily, performance degradation with multiple constraints in instructions, generation of toxic or biased outputs, and over-generalization leading to refusal of innocuous instructions.\"},\n",
    "47: {\"question\": \"What are some key factors to consider when building a successful multi-task instruction-following retrieval system as identified in the research?\",\n",
    "  \"gold_answer_research\": \"Some key factors to consider when building a successful multi-task instruction-following retrieval system include the need for cross-task interdependence for training a single retriever, the flexibility and zero-shot transfer enabled by instructions compared to task identifiers, and the elimination of the need for hosting multiple task-specific retrievers. Additionally, optimizing the mix and volume of instructional data for diverse tasks is crucial, as well as considering the impact of ranking strategy in data construction. Finally, the effectiveness of the dataset scale in retrieval and the importance of carefully designed negative samples should be taken into account for improved efficiency of instruction-following retrievers.\",\n",
    "  \"gold_answer_marketing\": \"Key factors to consider when building a successful multi-task instruction-following retrieval system include the effectiveness of the dataset scale in retrieval, the diversity in data and model scale, carefully designed negative samples, and the ability to adapt to new tasks via instructions.\"},\n",
    "48: {\"question\": \"What are the benefits of using retrieval-augmented techniques in multimodal language modeling, as demonstrated by the performance of the RA-CM3 model in the document?\",\n",
    "  \"gold_answer_research\": \"The benefits of using retrieval-augmented techniques in multimodal language modeling, as demonstrated by the performance of the RA-CM3 model, include significantly better training efficiency with less training compute, outperforming existing models by using less training data, compute, and parameters. The retrieval augmentation allows the model to focus on learning how to use retrieved documents in context, leading to improved accuracy in classification tasks. Additionally, the RA-CM3 model achieves strong performance in image and caption generation, surpassing existing models like DALL-E and Flamingo despite using fewer resources.\",\n",
    "  \"gold_answer_marketing\": \"The benefits of using retrieval-augmented techniques in multimodal language modeling, as demonstrated by the performance of the RA-CM3 model in the document, include outperforming existing models by using less training data, compute, and parameters, achieving significantly better training efficiency, and improving accuracy in k-shot classification tasks. Additionally, retrieval augmentation allows the model to focus on learning how to use retrieved documents in context, leading to stronger performance in tasks such as image and caption generation.\"},\n",
    "50: {\"question\": \"What methods are typically employed to create training data for embedding models that use task-specific instructions?\",\n",
    "  \"gold_answer_research\": \"To create training data for embedding models that use task-specific instructions, a common method is to combine datasets from different sources, such as the SuperNaturalInstructions dataset with existing collections designed for embedding training. The SuperNaturalInstructions dataset provides natural language instructions, which can be paired with positive and negative examples to form training samples. Additionally, for tasks like classification or similarity, training samples can be constructed by selecting text sequences associated with different classes or similarities. This diverse training data is essential for instruction-based finetuning, which enables the embedding model to learn from a wide range of tasks and domains.\",\n",
    "  \"gold_answer_marketing\": \"Training data for embedding models that use task-specific instructions is typically created by formulating a wide variety of tasks as text-to-text problems, distinguishing good/bad candidate outputs given an input text. This is done by combining datasets with natural language instructions and constructing positive and negative pairs for training.\"},\n",
    "51: {\"question\": \"Question: What are some of the challenges and innovations associated with fine-tuning large language models, and how does the approach discussed in the referenced text aim to address them?\",\n",
    "  \"gold_answer_research\": \"Some challenges associated with fine-tuning large language models include limited access to and manipulation of knowledge, lagging performance on knowledge-intensive tasks, and the need for provenance in decision-making and updating world knowledge. The approach discussed in the referenced text aims to address these challenges by utilizing Retrieval Augmented Generation (RAG), which involves retrieving relevant passages from a corpus to feed to the language model for improved performance in tasks such as question-answering and dialogue. This iterative approach focuses on improving alignment with user intent and fine-tuning models to control sentiment and improve response quality in various language tasks.\",\n",
    "  \"gold_answer_marketing\": \"The challenges with fine-tuning large language models include aligning them with user intent and controlling the quality of generated outputs. The approach discussed in the referenced text aims to address these challenges by using Retrieval Augmented Generation (RAG) to retrieve relevant passages from a corpus and feed them to the language model, improving alignment and performance.\"},\n",
    "52: {\"question\": \"What is a common technique used to address the outlier issue when applying block-wise k-bit quantization to input tensors, and how does it work?\",\n",
    "  \"gold_answer_research\": \"A common technique used to address the outlier issue when applying block-wise k-bit quantization to input tensors is to chunk the input tensor into blocks that are independently quantized, each with their own quantization constant. This approach involves dividing the input tensor into contiguous blocks of size B by flattening the tensor and slicing it into n blocks, where n is determined by the size of the blocks. Each block is then quantized independently using a quantization constant c, which helps prevent outlier values from causing performance degradation.\",\n",
    "  \"gold_answer_marketing\": \"A common technique used to address the outlier issue when applying block-wise k-bit quantization to input tensors is to chunk the input tensor into blocks that are independently quantized, each with their own quantization constant. This helps prevent performance degradation by reducing the impact of outliers on the quantization process.\"},\n",
    "54: {\"question\": \"What considerations or techniques are commonly implemented when setting up finetuning experiments for machine learning models?\",\n",
    "  \"gold_answer_research\": \"When setting up finetuning experiments for machine learning models, it is common to use a two-stage approach. The initial stage involves setting the initial parameters using a language modeling objective. This is followed by a supervised discriminative 'fine-tuning' stage to adapt these parameters to the target task. Additionally, it is typical to train all models using the Adam optimizer and a triangular learning rate scheduler with 10% warmup. Experimentation with different hyperparameters such as number of epochs, peak learning rate, and batch size is also conducted to optimize model performance. Finally, utilizing a mixture of datasets and balancing the sizes of datasets can help improve the robustness and generalization of the finetuned models.\",\n",
    "  \"gold_answer_marketing\": \"Considerations for setting up finetuning experiments for machine learning models commonly include using a language modeling objective for initial parameter setting and supervised discriminative fine-tuning for adapting parameters to the target task. Techniques such as hyperparameter search, Adam optimizer with triangular learning rate scheduler, and balancing dataset sizes through mixing strategies are also commonly implemented. Additionally, freezing some model layers during fine-tuning and incorporating negative examples for contrastive learning can be effective strategies.\"},\n",
    "55: {\"question\": \"What are the implications of the equivalence relation defined in the theoretical analysis of the DPO model for understanding the relationship between reward functions in reinforcement learning?\",\n",
    "  \"gold_answer_research\": \"The equivalence relation defined in the theoretical analysis of the DPO model implies that two reward functions are considered equivalent if they differ by a constant function. This means that the class of learned reward models is not constrained by this reparameterization, allowing for the exact recovery of the optimal policy. Understanding this relationship between reward functions in reinforcement learning helps in defining a unique reward function within each equivalence class, which is crucial for optimizing policies under existing models of human preferences. It also highlights the generality and flexibility in the reward model due to the proposed reparameterization.\",\n",
    "  \"gold_answer_marketing\": \"The equivalence relation defined in the theoretical analysis of the DPO model shows that two reward functions are considered equivalent if they differ by a fixed function. This implies that different reward functions can lead to the same optimal policy, allowing for flexibility in designing reward models in reinforcement learning.\"},\n",
    "59: {\"question\": \"Considering the structure and content of the provided text, what guidelines should be used to evaluate the effectiveness of a summary or chatbot response in this context?\",\n",
    "  \"gold_answer_research\": \"To evaluate the effectiveness of a summary or chatbot response in this context, guidelines should include assessing the faithfulness of the answer to the retrieved context, the relevance of the answer to the question, and the focus of the retrieved context. Additionally, consider using quality metrics such as answer relevancy to rank responses based on how directly they address the question and avoid redundant or incomplete information. Lastly, take into account the performance of different tasks such as summarization, citation prediction, and passage ranking to determine the overall effectiveness of the response.\",\n",
    "  \"gold_answer_marketing\": \"Answer: Evaluate based on faithfulness, answer relevance, and context relevance.\"},\n",
    "60: {\"question\": \"What are some recent methods and technologies that have been developed to enhance the capabilities and performance of natural language processing models?\",\n",
    "  \"gold_answer_research\": \"Recent methods and technologies developed to enhance natural language processing models include retrieval-augmented multimodal language modeling, which outperforms existing models with less training data and parameters. Another advancement is the use of feature learning in infinite-width neural networks to improve performance. Additionally, embedding techniques in NLP have been developed to map words or phrases to real number vectors, enhancing the model's understanding of language. These innovations have led to improvements in tasks like query reformulation, document ranking, and fine-tuning larger language models for various applications.\",\n",
    "  \"gold_answer_marketing\": \"Recent methods and technologies include retrieval-augmented language models, feature learning in infinite-width neural networks, and word embeddings.\"},\n",
    "61: {\"question\": \"What are some potential directions for future work mentioned in the document related to enhancing question-answering techniques for document-oriented tasks?\",\n",
    "  \"gold_answer_research\": \"One potential direction for future work mentioned in the document is the development of multi-modal approaches that incorporate table and figure information into GPT-4 question-answering for documents. Another direction is to incorporate question type in the PDFTriage approach to improve the efficiency and efficacy of the approach. Additionally, the document suggests further research in document-grounded, information-seeking question answering, which the dataset is designed to facilitate.\",\n",
    "  \"gold_answer_marketing\": \"Some potential future directions mentioned in the document include developing multi-modal approaches that incorporate table and figure information into question-answering for documents, and incorporating question type in the PDFTriage approach to improve efficiency and efficacy.\"},\n",
    "62: {\"question\": \"What information would you expect to find in section 2 of a document, based on the types of questions classified under Summarization?\",\n",
    "  \"gold_answer_research\": \"Based on the types of questions classified under Summarization, you would expect to find key takeaways, concise summaries, and specific content extraction related to different sections of the document in section 2. The section likely contains detailed summaries of specific parts of the document, along with structured metadata representation and instructions for summarizing the content effectively. It may also include guidelines for extracting specific information and rewriting text for clarity and conciseness.\",\n",
    "  \"gold_answer_marketing\": \"Based on the types of questions classified under Summarization, you would expect to find key takeaways, concise summaries, and specific content extraction related to the document in section 2.\"},\n",
    "63: {\"question\": \"What are the main advantages and attention mechanisms that contribute to the enhanced performance and efficiency of the newly introduced language model as compared to its predecessors?\",\n",
    "  \"gold_answer_research\": \"The main advantages of the newly introduced language model include utilizing retrieval-augmentation to incorporate external knowledge, which improves prediction accuracy. Additionally, the model employs attention mechanisms that allow for better understanding of dependencies between source and target sequences, leading to more informed predictions. These attention mechanisms have been extended from machine translation to various other fields, enhancing the model's adaptability and performance across different tasks. Finally, the model's use of self-attention mechanisms enables better contextual representation learning, parallelization, and modeling of longer intra-token relations, improving efficiency and performance compared to previous models.\",\n",
    "  \"gold_answer_marketing\": \"The main advantages of the newly introduced language model include the use of retrieval-augmented mechanisms, attention mechanisms, and context representation learning, which contribute to enhanced performance and efficiency compared to its predecessors.\"},\n",
    "64: {\"question\": \"What criteria are used to assess the quality of recommendations provided by different language models in a comparison study?\",\n",
    "  \"gold_answer_research\": \"In a comparison study of language models, criteria such as sentence relevance, lexical accuracy, and contextual understanding are used to assess the quality of recommendations. Different tasks may benefit from different evaluation measures, such as STRINC, LEXICAL, and CXMI. Additionally, template selection plays a vital role in the quality of recommendations, with deliberate template design being important for tasks like query suggestion. The overall quality of recommendations is often judged using a Likert scale, along with metadata collection for each model output.\",\n",
    "  \"gold_answer_marketing\": \"The criteria used to assess the quality of recommendations provided by different language models in a comparison study include comparing to human-created benchmarks, examining intrinsic character, comparing two models, investigating rate of learning, and analyzing learning curves.\"},\n",
    "65: {\"question\": \"What approaches have been proposed to enhance the task performance of language models while considering the trade-offs such as runtime efficiency, robustness to irrelevant context, and attribution quality?\",\n",
    "  \"gold_answer_research\": \"Several approaches have been proposed to enhance the task performance of language models while considering trade-offs. These include using compression and selective augmentation methods to decrease the propensity of models to generate toxic or biased outputs. Adversarial setups have been suggested where labelers find worst-case behaviors of the model and add them to the dataset. Additionally, models like BART and T5 leverage bi-directional attention to achieve stronger performance on both discriminative and generative tasks. These methods aim to balance model performance with considerations such as runtime efficiency, robustness to irrelevant context, and attribution quality.\",\n",
    "  \"gold_answer_marketing\": \"Approaches proposed to enhance language model task performance include compression and selective augmentation, adversarial set-ups for labeling worst-case behaviors, retrieval-augmented models, and extending existing models to enable length extrapolation while maintaining quality.\"},\n",
    "67: {\"question\": \"What metrics are commonly used to compare the performance of language models in various tasks, as outlined in an experimental results table?\",\n",
    "  \"gold_answer_research\": \"Common metrics used to compare the performance of language models in various tasks, as outlined in an experimental results table, include Exact Match and Unigram F1. These metrics have become standard in evaluating language models. Additionally, other metrics such as BLEU score, FactScore (factuality), precision, and recall are also commonly used to assess the performance of language models across different tasks. It is important to consider a variety of metrics to get a comprehensive understanding of the effectiveness of a language model in different contexts.\",\n",
    "  \"gold_answer_marketing\": \"The metrics commonly used to compare the performance of language models in various tasks are Exact Match and Unigram F1.\"},\n",
    "69: {\"question\": \"What is the role of manual assessment in the validation of language model predictions according to the text provided?\",\n",
    "  \"gold_answer_research\": \"Manual assessment plays a crucial role in the validation of language model predictions. The engineers evaluate the quality of model outputs by having labelers rate them on test sets consisting of prompts from held-out customers. This manual assessment helps ensure that the models are aligned with a broad distribution of language tasks and can identify any behavioral issues that may arise from misalignment. Additionally, human annotators find that certain reflection token predictions are aligned with their assessments, providing valuable insights into the accuracy and effectiveness of the models.\",\n",
    "  \"gold_answer_marketing\": \"Answer: Manual assessment plays a key role in evaluating the quality of language model predictions by having labelers rate the model outputs and comparing them to prompts from held-out customers.\"},\n",
    "70: {\"question\": \"What are the general steps outlined for training a language model in the document, and how is the training data for the generator language model collected and utilized?\",\n",
    "  \"gold_answer_research\": \"The document outlines the general steps for training a language model, including incorporating retrieved documents into the main input sequence and optimizing the loss function to train the generator. The training data for the generator language model is collected through various techniques such as supervised fine-tuning, critic learning, and custom retrievers for downstream tasks. The collected data is used to train the generator on specific tasks like summarization, machine reading comprehension, and natural language to SQL translation, improving performance on those tasks.\",\n",
    "  \"gold_answer_marketing\": \"The general steps for training a language model include fine-tuning on specific datasets, filtering pretraining data, and using critic learning. Training data for the generator language model is collected from open-access NLP papers and used for downstream conditional text generation tasks.\"},\n",
    "73: {\"question\": \"What are the three main categories used to refine language model abilities in understanding and executing search tasks according to the given document?\",\n",
    "  \"gold_answer_research\": \"The three main categories used to refine language model abilities in understanding and executing search tasks are query understanding, document understanding, and query-document relationship understanding. Tasks within these categories focus on interpreting queries, comprehending documents, and understanding the relationships between queries and documents. This approach aims to enhance the models' performance in interpreting and responding to search-related instructions effectively, improving their utility in complex information retrieval scenarios.\",\n",
    "  \"gold_answer_marketing\": \"The three main categories used to refine language model abilities in understanding and executing search tasks are query understanding, document understanding, and query-document relationship understanding.\"},\n",
    "74: {\"question\": \"What are some of the emerging research topics and challenges in the field of natural language processing and information retrieval according to recent academic conferences and publications?\",\n",
    "  \"gold_answer_research\": \"Recent academic conferences and publications have highlighted emerging research topics and challenges in natural language processing and information retrieval. Some key areas of focus include efficient retrieval augmented generation, unsupervised dense information retrieval with contrastive learning, citation-informed transformers, and knowledge refinement via interaction between search engines and large language models. Additionally, challenges such as zero-shot retrieval, semantic search using GPT sentence embeddings, and prompt-based effective input reformulation for legal case retrieval have been identified as important research directions. These topics reflect the ongoing advancements and complexities in the field, driving innovation and progress in NLP and IR research.\",\n",
    "  \"gold_answer_marketing\": \"Some emerging research topics and challenges in the field of natural language processing and information retrieval include efficient generation from unstructured knowledge, semantic code search evaluation, unsupervised dense information retrieval, context-aware document term weighting, knowledge refinement through interaction with large language models, and investigating the effectiveness of large language models in search re-ranking.\"},\n",
    "75: {\"question\": \"Question: How do models with different fine-tuning strategies compare in terms of accuracy and F1 score for fact verification tasks?\",\n",
    "  \"gold_answer_research\": \"Models with different fine-tuning strategies are compared in terms of accuracy and F1 score for fact verification tasks. The introduction of LLMs has led to notable developments, with some studies leveraging prompting methods to apply LLMs in IR tasks. However, not all LLMs consistently outperform fine-tuned smaller models. For example, RankGPT based on gpt-3.5-turbo underperforms monoBERT in certain scenarios. Fine-tuning is not strictly necessary for models like GPT3, which has been evaluated on closed book question answering tasks without any updates or fine-tuning.\",\n",
    "  \"gold_answer_marketing\": \"Models with different fine-tuning strategies have shown mixed results in terms of accuracy and F1 score for fact verification tasks. Some studies have found that large language models (LLMs) outperform smaller fine-tuned models, while others have reported inconsistent performance. Factors such as task complexity and the need for prompt methods to apply LLMs in information retrieval tasks can also impact the comparison.\"},\n",
    "76: {\"question\": \"What components does a fact verification task typically involve in order to assess the accuracy of a given statement?\",\n",
    "  \"gold_answer_research\": \"A fact verification task typically involves assessing the relationship between a claim and the evidence provided, analyzing if there is enough information for a conclusive judgment. This task requires a detailed understanding of the claim and evidence to determine if it is supported or refuted. The use of performance metrics based on including gold answers in model generations instead of exact matching can help search engines deliver accurate and relevant results. Additionally, incorporating lexical measures and verification functions can aid in determining the accuracy of statements.\",\n",
    "  \"gold_answer_marketing\": \"A fact verification task typically involves assessing the relationship between a claim and supporting evidence to determine accuracy.\"},\n",
    "78: {\"question\": \"What are the key factors that determine the performance of HALO-aligned models compared to non-HALO models, according to the results presented in the analysis?\",\n",
    "  \"gold_answer_research\": \"According to the analysis presented, the key factors that determine the performance of HALO-aligned models compared to non-HALO models include the specific alignment method used (such as DPO and PPO variant), the model size (significant gap at 13B+ model sizes), and the ability to match or exceed the generation quality of SFT target sequences. Additionally, the study suggests that the cost of increasing model alignment is modest relative to pretraining, and that the modeling of human biases in HALOs may have practical benefits in improving overall performance.\",\n",
    "  \"gold_answer_marketing\": \"The key factor that determines the performance of HALO-aligned models compared to non-HALO models is the model size, with HALO-aligned models generally outperforming non-HALO models at larger sizes (13B+ model sizes).\"},\n",
    "80: {\"question\": \"How does the performance of KTO compare to DPO in model alignment, and what are the potential implications for data usage and training efficiency?\",\n",
    "  \"gold_answer_research\": \"Based on the provided data and experiments, KTO consistently outperforms DPO in model alignment, even with restrictions such as using only one output per input. This suggests that KTO can achieve higher win rates and improve performance across various benchmarks compared to DPO. The implications of this performance difference include the ability to achieve quality generation results with significantly fewer desirable examples, potentially leading to more efficient data usage and training processes. This indicates that KTO may offer a more efficient and effective approach to model alignment compared to DPO.\",\n",
    "  \"gold_answer_marketing\": \"KTO outperforms DPO in model alignment with up to 90% fewer examples. This suggests that KTO can achieve high performance even with imbalanced data, potentially leading to more efficient training processes.\"},\n",
    "81: {\"question\": \"What are some common approaches to building an open-domain question answering system?\",\n",
    "  \"gold_answer_research\": \"Some common approaches to building an open-domain question answering system include using the RAG model, which minimizes the negative log-likelihood of answers, and comparing it to extractive QA paradigms that rely on non-parametric knowledge retrieval. Another approach is to incorporate question rewriting techniques to make open-domain QA more conversational. Additionally, utilizing datasets like QASPER, which contain questions requiring complex reasoning, can improve the performance of the system. References to papers by Anantha et al. and Asai et al. provide further insights into building ODQA systems.\",\n",
    "  \"gold_answer_marketing\": \"Common approaches to building an open-domain question answering system include using retrieval over a knowledge base and incorporating the retrieved content as part of the prompt. Other methods involve pretraining models on large amounts of text data and fine-tuning them for question answering tasks.\"},\n",
    "82: {\"question\": \"What is the difference between open-book and closed-book question answering?\",\n",
    "  \"gold_answer_research\": \"Open-book question answering involves the use of external sources of knowledge, such as Wikipedia, to retrieve information and generate a response. In contrast, closed-book question answering relies on pre-trained language models that have memorized factual knowledge within their parameters to generate responses without explicit context. Closed-book QA can be seen as analogous to a closed-book exam where no external resources are allowed. The key distinction lies in the reliance on external knowledge sources for open-book QA versus internal memorized knowledge for closed-book QA.\",\n",
    "  \"gold_answer_marketing\": \"Open-book question answering involves using external sources of knowledge to answer questions, while closed-book question answering relies on pre-trained language models to provide answers without explicit context.\"},\n",
    "84: {\"question\": \"What are the basic components of the Retriever-Reader framework in open-domain QA?\",\n",
    "  \"gold_answer_research\": \"The basic components of the Retriever-Reader framework in open-domain QA include a retriever model, which fetches relevant information based on input prompts efficiently using FAISS. The retriever component is responsible for retrieving contextually relevant documents or evidence blocks based on the input question. The reader component then processes this retrieved information to generate answers to the questions posed. This framework combines information retrieval and machine reading comprehension to achieve state-of-the-art results in open-domain question answering tasks.\",\n",
    "  \"gold_answer_marketing\": \"The basic components of the Retriever-Reader framework in open-domain QA are the retriever and the reader components, which can be set up and trained independently or jointly trained end-to-end. The retriever component automatically fetches relevant information based on input prompts, while the reader component processes and comprehends the retrieved information to answer questions.\"},\n",
    "85: {\"question\": \"How is the TF-IDF model used in question answering retrieval systems?\",\n",
    "  \"gold_answer_research\": \"In question answering retrieval systems, the TF-IDF model is used to represent queries and documents as bag-of-word vectors with terms weighted by term frequency multiplied by inverse document frequency. This allows for efficient non-learning-based search engine operations based on the vector space model. The TF-IDF model helps in calculating the relevance of documents to queries by measuring the importance of terms in the context of the entire document collection. This classic information retrieval approach aids in retrieving relevant information to answer questions accurately and efficiently.\",\n",
    "  \"gold_answer_marketing\": \"The TF-IDF model is used in question answering retrieval systems to weight terms in queries and documents based on their importance in determining relevance.\"},\n",
    "86: {\"question\": \"Can neural networks enhance the process of information retrieval in QA systems?\",\n",
    "  \"gold_answer_research\": \"Neural networks, such as MLP, LSTM, and bidirectional LSTM, can be used to learn dense representations of text for information retrieval in QA systems. These approaches, known as 'Neural IR', are a new category of methods that can improve performance in retrieval problems. The introduction of neural retrievers in recent QA literature has shown to outperform traditional word-similarity-based architectures, such as BM25, and can scale to handle knowledge-grounded dialogue tasks effectively. Additionally, incorporating pre-trained retrievers in QA systems has been shown to enhance the performance of generative language models.\",\n",
    "  \"gold_answer_marketing\": \"Yes, neural networks can enhance the process of information retrieval in QA systems by improving performance in open-domain QA tasks and enabling the generation of more accurate answers.\"},\n",
    "87: {\"question\": \"What is the importance of fine-tuning in the context of QA data for open-domain question answering models?\",\n",
    "  \"gold_answer_research\": \"Fine-tuning is important in the context of QA data for open-domain question answering models because it allows the model to adapt and improve its performance on specific QA datasets. By fine-tuning the model with common QA datasets, engineers can optimize the model's ability to answer questions accurately. However, there is a concern about the significant overlap between questions in the train and test sets of public QA datasets, which could affect the generalization ability of the fine-tuned models. Engineers should carefully consider this overlap and potentially explore ways to mitigate its impact during the fine-tuning process to ensure the model's effectiveness in real-world applications.\",\n",
    "  \"gold_answer_marketing\": \"Fine-tuning is important in the context of QA data for open-domain question answering models to improve search task performance and the ability to generalize to unseen datasets.\"},\n",
    "88: {\"question\": \"How does pre-training with tasks like the Inverse Cloze Task benefit open-domain question answering models?\",\n",
    "  \"gold_answer_research\": \"Pre-training with tasks like the Inverse Cloze Task benefits open-domain question answering models by improving the retrieval process over a knowledge base. By predicting the context given a sentence, the model can better understand the relationship between the question and the evidence. This approach helps in incorporating retrieved content effectively into the prompt, leading to higher accuracy in the question answering task. Additionally, using models pretrained with ICT can enhance the overall performance of the QA system by providing a better understanding of the context.\",\n",
    "  \"gold_answer_marketing\": \"Pre-training with tasks like the Inverse Cloze Task benefits open-domain question answering models by improving retrieval and generation steps, ultimately enhancing the accuracy of the process.\"},\n",
    "89: {\"question\": \"What is the main goal of prompt engineering in language models?\",\n",
    "  \"gold_answer_research\": \"The main goal of prompt engineering in language models is to effectively steer the behavior of the model towards desired outcomes without updating the model weights. This is achieved by composing and formatting prompts in a way that maximizes the model's performance on a specific task. Prompt engineering involves treating prompts as trainable parameters and optimizing them directly on the embedding space through methods like AutoPrompt, Prefix-Tuning, P-tuning, and Prompt-Tuning. The ultimate aim is to enhance the model's performance and alignment with user-defined tasks.\",\n",
    "  \"gold_answer_marketing\": \"The main goal of prompt engineering in language models is to steer the behavior of the model for desired outcomes without updating the model weights.\"},\n",
    "91: {\"question\": \"What are some known biases that can affect the performance of few-shot classification in LLMs?\",\n",
    "  \"gold_answer_research\": \"Some known biases that can affect the performance of few-shot classification in LLMs include majority label bias, recency bias, and common token bias. Majority label bias occurs when the distribution of labels among examples is unbalanced, recency bias refers to the tendency for the model to repeat the label at the end, and common token bias indicates that LLM tends to produce common tokens more often than rare tokens. These biases can contribute to high variance in few-shot classification tasks and may impact the model's ability to generalize effectively.\",\n",
    "  \"gold_answer_marketing\": \"Some known biases that can affect the performance of few-shot classification in LLMs are majority label bias, recency bias, and common token bias.\"},\n",
    "92: {\"question\": \"Why might increasing model size not reduce variance in model performance with varying prompts?\",\n",
    "  \"gold_answer_research\": \"Increasing model size may not necessarily reduce variance in model performance with varying prompts because the model's ability to generalize and adapt to different prompts is not solely dependent on its size. Factors such as the quality and relevance of the training examples, the learning rate or schedule, and the model's sensitivity to different hyperparameters can also play a significant role in determining performance variability. Additionally, the complexity of the task or dataset being used for training can impact how effectively the model scales with size. It is essential to consider these factors holistically when optimizing model performance rather than relying solely on increasing model size.\",\n",
    "  \"gold_answer_marketing\": \"Increasing model size may not reduce variance in model performance with varying prompts because the same order of prompts may work well for one model but poorly for another. Additionally, when the validation set is limited, choosing the order of prompts that prevents the model from producing extremely unbalanced predictions or being overconfident can also affect performance.\"},\n",
    "93: {\"question\": \"What is the benefit of instruction-based finetuning in language models?\",\n",
    "  \"gold_answer_research\": \"Instruction-based finetuning improves models' ability to generalize to unseen domains and tasks by providing task-specific representations that can be used for many downstream language tasks without additional training. This method also allows pretrained language models to follow instructions provided in prompts, enabling them to generate the desired output given specific inputs. Additionally, instruction finetuning helps transform raw pretrained LLMs into chatbot-like models, making finetuning more accessible and common, particularly for researchers with limited resources. Overall, the benefit of instruction-based finetuning is improved model performance, enhanced generalizability, and reduced communication costs in aligning with human intentions.\",\n",
    "  \"gold_answer_marketing\": \"The benefit of instruction-based finetuning in language models is improved ability to generalize to unseen domains and tasks, without the need for additional training.\"},\n",
    "94: {\"question\": \"Can you describe a situation where retrieval-based methods would be necessary to enhance language model performance?\",\n",
    "  \"gold_answer_research\": \"Retrieval-based methods are necessary to enhance language model performance in scenarios where the model needs to generate accurate and informative responses for entity-rich queries, such as 'George Washington standing in front of the Eiffel Tower.' In such cases, incorporating a retrieval module can provide additional context and relevant information to improve the model's understanding and generation of the desired output. Additionally, retrieval-based methods are crucial for question answering tasks, where the model needs to access external knowledge sources to provide accurate and comprehensive answers. By utilizing retrieval mechanisms, the language model can benefit from a wider range of information and improve its performance in handling complex and ambiguous queries effectively.\",\n",
    "  \"gold_answer_marketing\": \"Retrieval-based methods are necessary to enhance language model performance in tasks like question answering, where incorporating additional information from external sources can improve the model's ability to generate accurate and relevant responses.\"},\n",
    "95: {\"question\": \"What is the Chain-of-Thought prompting technique and for which types of tasks is it particularly beneficial?\",\n",
    "  \"gold_answer_research\": \"Chain-of-Thought (CoT) prompting is a technique that generates reasoning chains or rationales step by step to lead to a final answer, benefiting complicated reasoning tasks using large models with more than 50B parameters. It can be implemented through iterative Monte Carlo search methods or through a three-step process called augment-prune-select. CoT is particularly beneficial for enhancing model performance on complex tasks by decomposing them into smaller and simpler steps, shedding light on the model's thinking process. Task decomposition in CoT can be done with simple prompting, task-specific instructions, or human inputs.\",\n",
    "  \"gold_answer_marketing\": \"Chain-of-Thought (CoT) prompting is a technique that generates reasoning chains or rationales step by step to lead to a final answer. It is particularly beneficial for complicated reasoning tasks when using large models with more than 50B parameters. Simple tasks only benefit slightly from CoT prompting.\"},\n",
    "96: {\"question\": \"How do augmented language models with external tools differ from regular models in functionality?\",\n",
    "  \"gold_answer_research\": \"Augmented language models with external tools, such as TALM and Toolformer, are fine-tuned to learn how to use external tool APIs, expanding their capabilities beyond traditional language processing tasks. These models are trained to incorporate external tool API calls in order to improve the quality of their outputs, allowing them to perform tasks like speech recognition, machine translation, and information retrieval more effectively. By leveraging external tools, these models have the ability to access and utilize a wider range of resources and functionalities, enhancing their overall performance and versatility compared to regular language models.\",\n",
    "  \"gold_answer_marketing\": \"Augmented language models with external tools differ from regular models by fine-tuning a LM to use external tool APIs, expanding the dataset to improve model outputs and enhancing tasks like speech recognition, machine translation, and natural language generation.\"},\n",
    "97: {\"question\": \"What can be inferred about the utilization of attention in neural networks?\",\n",
    "  \"gold_answer_research\": \"Attention mechanisms in neural networks play a crucial role in allowing models to focus on specific parts of input data when making predictions or generating outputs. By assigning importance weights to different elements, such as pixels in an image or words in a sentence, attention helps the model to attend to relevant information and make more accurate predictions. The use of attention can improve the interpretability of neural networks by showing which parts of the input data are being focused on during the prediction process. Additionally, attention mechanisms, like multi-head attention, can enhance model performance by allowing the model to jointly attend to information from different representation subspaces at different positions.\",\n",
    "  \"gold_answer_marketing\": \"Attention in neural networks allows the model to focus on specific parts of input data, such as images or text, in order to make predictions or generate output. It helps the model to learn relationships and correlations between different elements and improve performance in tasks like image captioning or language translation.\"},\n",
    "101: {\"question\": \"Can the use of attention mechanisms in deep learning models be applied to both machine translation and computer vision?\",\n",
    "  \"gold_answer_research\": \"Yes, attention mechanisms in deep learning models have shown success in both machine translation and computer vision tasks. In machine translation, attention allows the model to capture dependencies between source and target sequences regardless of distance, leading to improved translation quality. Similarly, in computer vision, attention mechanisms have been used to focus on relevant parts of an image during caption generation, showcasing the ability to handle details and global dependencies effectively. Therefore, utilizing attention in both domains can enhance the performance of deep learning models significantly.\",\n",
    "  \"gold_answer_marketing\": \"Yes, attention mechanisms in deep learning models can be applied to both machine translation and computer vision.\"},\n",
    "102: {\"question\": \"What are the potential benefits of incorporating self-attention mechanisms into Generative Adversarial Networks (GANs)?\",\n",
    "  \"gold_answer_research\": \"Incorporating self-attention mechanisms into GANs can help the generator and discriminator better model relationships between spatial regions, leading to improved generation of detailed and realistic images. This is particularly useful for capturing global dependencies and enhancing the performance of transformer architectures. Additionally, self-attention can enable the model to assess its own predictions after each generated segment, allowing for customizable decoding algorithms to meet specific constraints or user preferences. Overall, self-attention in GANs can enhance detail handling and overall performance.\",\n",
    "  \"gold_answer_marketing\": \"Incorporating self-attention mechanisms into GANs can help the generator and discriminator better model relationships between spatial regions, leading to improved performance in handling details and capturing global dependencies.\"},\n",
    "103: {\"question\": \"How does the transformer model variate from traditional sequence-aligned recurrent architectures?\",\n",
    "  \"gold_answer_research\": \"The transformer model differs from traditional sequence-aligned recurrent architectures by not having a recurrent or convolutional structure. Instead, it heavily relies on self-attention mechanisms for processing sequences. This lack of recurrence and convolution, even with positional encoding, weakly incorporates sequential order, which can be a drawback for tasks sensitive to positional dependencies. Additionally, the transformer's architecture includes embedding layers, sinusoid-wave-based positional encoding, and softmax and linear layers in the final decoder output to maintain position information and facilitate processing of long sequences efficiently.\",\n",
    "  \"gold_answer_marketing\": \"The transformer model differs from traditional sequence-aligned recurrent architectures by not having a recurrent or convolutional structure, and instead making heavy use of self-attention. This allows for handling very long sequences efficiently and achieving better performance on tasks involving long texts.\"},\n",
    "104: {\"question\": \"What implications does the concept of a Neural Turing Machine have for the theoretical power of neural networks?\",\n",
    "  \"gold_answer_research\": \"The concept of a Neural Turing Machine (NTM) expands the theoretical power of neural networks by incorporating external memory storage, allowing for more complex computations and tasks. This mimics the Turing machine tape, enabling the neural network to control operation heads for reading and writing to the tape. However, the finite memory in NTM suggests it may resemble more of a 'Neural von Neumann Machine,' limiting its mathematical limitlessness seen in traditional Turing machines. Overall, the addition of external memory in NTM enhances the capabilities and potential applications of neural networks in solving more advanced problems.\",\n",
    "  \"gold_answer_marketing\": \"The concept of a Neural Turing Machine suggests that neural networks can be equipped with external memory storage for more complex operations, potentially increasing their theoretical power.\"},\n",
    "}\n",
    "\n",
    "\n",
    "test_questions = {\n",
    "4: {\"question\": \"When was the transformer architecture introduced, and by which organization?\"},\n",
    "5: {\"question\": \"How has the accessibility of powerful language models, such as GPT-3 and GPT-4, been controlled by their developers?\"},\n",
    "6: {\"question\": \"What benchmarks or ratings are used to compare the capabilities of different language models?\"},\n",
    "10: {\"question\": \"What are some of the primary applications for language models in technology and computing?\"},\n",
    "14: {\"question\": \"How are language models typically evaluated and what benchmarks are used for this purpose?\"},\n",
    "15: {\"question\": \"What datasets are available for evaluating language processing systems?\"},\n",
    "21: {\"question\": \"What collaborations with other companies have contributed to the development of Claude's capabilities?\"},\n",
    "26: {\"question\": \"According to DeepMind, how should the number of training tokens change relative to the model size?\"},\n",
    "29: {\"question\": \"How do the sizes of models in the Gopher family range?\"},\n",
    "31: {\"question\": \"What type of model architecture do the Gopher and Chinchilla families belong to?\"},\n",
    "32: {\"question\": \"Can you name the author who wrote the novels A Farewell to Arms and The Sun Also Rises?\"},\n",
    "37: {\"question\": \"What are the key advantages of InstructGPT models over GPT-3 models according to the findings in the research?\"},\n",
    "40: {\"question\": \"What metrics are used to compare the performance of different models on training and validation splits according to the document provided?\"},\n",
    "42: {\"question\": \"What types of evaluation metrics are commonly used to assess the accuracy of answers in AI-driven question and answer datasets?\"},\n",
    "49: {\"question\": \"What factors contribute to the performance improvement in retrieval-augmented language models compared to non-retrieval-augmented models?\"},\n",
    "56: {\"question\": \"What are the benchmarks used to evaluate the performance of the Deep Policy Optimization (DPO) method compared to other preference learning algorithms in the document provided?\"},\n",
    "57: {\"question\": \"What methodologies have been evaluated for training language models to align with human preferences, and how do they compare in terms of effectiveness?\"},\n",
    "58: {\"question\": \"What methods have been discussed in the literature for improving the alignment of language models with human preferences or feedback?\"},\n",
    "66: {\"question\": \"What are some of the evaluation metrics used for assessing different types of text generation tasks presented in the study?\"},\n",
    "68: {\"question\": \"Consider a document related to research in natural language processing or artificial intelligence. Can you name some of the recent topics or methods that have been discussed or introduced in the field according to the document?\"},\n",
    "71: {\"question\": \"What is the significance of using reflection tokens in a model like SELF-RAG?\"},\n",
    "72: {\"question\": \"How does the inclusion of selected context as opposed to appending all retrieved text spans impact computational cost during both training and inference times in language model generation tasks?\"},\n",
    "77: {\"question\": \"What are the benefits of modeling human biases in Human-Aware Loss Optimizations (HALOs), and how do they compare to non-HALOs on the same datasets?\"},\n",
    "79: {\"question\": \"What are the modifications made to the traditional Kahneman-Tversky model to adapt it for optimizing language model performance?\"},\n",
    "83: {\"question\": \"How does a model's ability to answer questions relate to its exposure to specific types of questions during training?\"},\n",
    "90: {\"question\": \"How can adding examples to a prompt affect the performance of language models?\"},\n",
    "98: {\"question\": \"What are the main components of a Neural Turing Machine (NTM) architecture?\"},\n",
    "99: {\"question\": \"How might a seq2seq model's limitations be addressed in natural language processing tasks?\"},\n",
    "100: {\"question\": \"What differentiates hard attention from soft attention in image processing algorithms?\"},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's run the queries now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Transformer model was proposed in a paper titled \"Attention is All You Need\" in June 2017 by researchers at Google Brain, led by Ashish Vaswani et al.\n"
     ]
    }
   ],
   "source": [
    "my_question = \"When was the transformer architecture introduced, and by which organization?\"\n",
    "# my_question = \"which segment of Audi's car was named as Ur-S4?\"\n",
    "# my_question = \"is Bank of America Tower taller than empire state building?\"\n",
    "\n",
    "# get_top_passages(my_question)\n",
    "\n",
    "response = uncompiled_rag(my_question)\n",
    "print(response.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To inspect the prompt internally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Answer questions with short factoid answers.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: may or maynot contain relevant facts or answer keywords\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: an answer between 10 to 20 words\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «In this work, we proposed a new position embedding method that incorporates explicit relative position dependency in\n",
      "self-attention to enhance the performance of transformer architectures. Our theoretical analysis indicates that relative\n",
      "position can be naturally formulated using vector production in self-attention, with absolution position information\n",
      "being encoded through a rotation matrix. In addition, we mathematically illustrated the advantageous properties of\n",
      "the proposed method when applied to the Transformer. Finally, experiments on both English and Chinese benchmark\n",
      "datasets demonstrate that our method encourages faster convergence in pre-training. The experimental results also show\n",
      "that our proposed RoFormer can achieve better performance on long texts task.\n",
      "References»\n",
      "[2] «datasets show that the enhanced transformer with rotary position embedding, namely RoFormer, can give better\n",
      "performance compared to baseline alternatives and thus demonstrates the efficacy of the proposed RoPE.\n",
      "In brief, our contributions are three-folds as follows:\n",
      "• We investigated the existing approaches to the relative position encoding and found that they are mostly\n",
      "built based on the idea of the decomposition of adding position encoding to the context representations. We\n",
      "introduce a novel method, namely Rotary Position Embedding(RoPE), to leverage the positional information\n",
      "into the learning process of PLMS. The key idea is to encode relative position by multiplying the context\n",
      "representations with a rotation matrix with a clear theoretical interpretation.»\n",
      "[3] «ROFORMER: ENHANCED TRANSFORMER WITH ROTARY\n",
      "POSITION EMBEDDING\n",
      "Jianlin Su\n",
      "Zhuiyi Technology Co., Ltd.\n",
      "Shenzhen\n",
      "bojonesu@wezhuiyi.com\n",
      "Yu Lu\n",
      "Zhuiyi Technology Co., Ltd.\n",
      "Shenzhen\n",
      "julianlu@wezhuiyi.com\n",
      "Shengfeng Pan\n",
      "Zhuiyi Technology Co., Ltd.\n",
      "Shenzhen\n",
      "nickpan@wezhuiyi.com\n",
      "Ahmed Murtadha\n",
      "Zhuiyi Technology Co., Ltd.\n",
      "Shenzhen\n",
      "mengjiayi@wezhuiyi.com\n",
      "Bo Wen\n",
      "Zhuiyi Technology Co., Ltd.\n",
      "Shenzhen\n",
      "brucewen@wezhuiyi.com\n",
      "Yunfeng Liu\n",
      "Zhuiyi Technology Co., Ltd.\n",
      "Shenzhen\n",
      "glenliu@wezhuiyi.com\n",
      "November 9, 2023\n",
      "ABSTRACT\n",
      "Position encoding recently has shown effective in the transformer architecture. It enables valuable\n",
      "supervision for dependency modeling between elements at different positions of the sequence. In»\n",
      "\n",
      "Question: When was the transformer architecture introduced, and by which organization?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m Context: The provided context discusses a new position embedding method (RoFormer) that enhances Transformer architectures with rotary position encoding. However, it does not mention when or where the original transformer architecture was introduced. \n",
      "\n",
      "Question: When was the transformer architecture introduced and by which organization? \n",
      "Answer: The Transformer model was proposed in a paper titled \"Attention is All You Need\" in June 2017 by researchers at Google Brain, led by Ashish Vaswani et al.\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\nAnswer questions with short factoid answers.\\n\\n---\\n\\nFollow the following format.\\n\\nContext: may or maynot contain relevant facts or answer keywords\\n\\nQuestion: ${question}\\n\\nReasoning: Let\\'s think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: an answer between 10 to 20 words\\n\\n---\\n\\nContext:\\n[1] «In this work, we proposed a new position embedding method that incorporates explicit relative position dependency in\\nself-attention to enhance the performance of transformer architectures. Our theoretical analysis indicates that relative\\nposition can be naturally formulated using vector production in self-attention, with absolution position information\\nbeing encoded through a rotation matrix. In addition, we mathematically illustrated the advantageous properties of\\nthe proposed method when applied to the Transformer. Finally, experiments on both English and Chinese benchmark\\ndatasets demonstrate that our method encourages faster convergence in pre-training. The experimental results also show\\nthat our proposed RoFormer can achieve better performance on long texts task.\\nReferences»\\n[2] «datasets show that the enhanced transformer with rotary position embedding, namely RoFormer, can give better\\nperformance compared to baseline alternatives and thus demonstrates the efficacy of the proposed RoPE.\\nIn brief, our contributions are three-folds as follows:\\n• We investigated the existing approaches to the relative position encoding and found that they are mostly\\nbuilt based on the idea of the decomposition of adding position encoding to the context representations. We\\nintroduce a novel method, namely Rotary Position Embedding(RoPE), to leverage the positional information\\ninto the learning process of PLMS. The key idea is to encode relative position by multiplying the context\\nrepresentations with a rotation matrix with a clear theoretical interpretation.»\\n[3] «ROFORMER: ENHANCED TRANSFORMER WITH ROTARY\\nPOSITION EMBEDDING\\nJianlin Su\\nZhuiyi Technology Co., Ltd.\\nShenzhen\\nbojonesu@wezhuiyi.com\\nYu Lu\\nZhuiyi Technology Co., Ltd.\\nShenzhen\\njulianlu@wezhuiyi.com\\nShengfeng Pan\\nZhuiyi Technology Co., Ltd.\\nShenzhen\\nnickpan@wezhuiyi.com\\nAhmed Murtadha\\nZhuiyi Technology Co., Ltd.\\nShenzhen\\nmengjiayi@wezhuiyi.com\\nBo Wen\\nZhuiyi Technology Co., Ltd.\\nShenzhen\\nbrucewen@wezhuiyi.com\\nYunfeng Liu\\nZhuiyi Technology Co., Ltd.\\nShenzhen\\nglenliu@wezhuiyi.com\\nNovember 9, 2023\\nABSTRACT\\nPosition encoding recently has shown effective in the transformer architecture. It enables valuable\\nsupervision for dependency modeling between elements at different positions of the sequence. In»\\n\\nQuestion: When was the transformer architecture introduced, and by which organization?\\n\\nReasoning: Let\\'s think step by step in order to\\x1b[32m Context: The provided context discusses a new position embedding method (RoFormer) that enhances Transformer architectures with rotary position encoding. However, it does not mention when or where the original transformer architecture was introduced. \\n\\nQuestion: When was the transformer architecture introduced and by which organization? \\nAnswer: The Transformer model was proposed in a paper titled \"Attention is All You Need\" in June 2017 by researchers at Google Brain, led by Ashish Vaswani et al.\\x1b[0m\\n\\n\\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama_model.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADVANCED (Using teleprompters to train the CoT model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = []\n",
    "for index in validation_questions_answers:\n",
    "    dataset_train.append(dspy.Example(question=validation_questions_answers[index]['question'], answer=validation_questions_answers[index]['gold_answer_research']).with_inputs(\"question\"))\n",
    "    if len(dataset_train) > 14:\n",
    "        break\n",
    "\n",
    "dataset_eval = []\n",
    "\n",
    "for index in validation_questions_answers:\n",
    "    if index > 20:\n",
    "        dataset_eval.append(dspy.Example(question=validation_questions_answers[index]['question'], answer=validation_questions_answers[index]['gold_answer_research']).with_inputs(\"question\"))\n",
    "    if len(dataset_eval) > 4:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 15/15 [16:19<00:00, 65.32s/it]\n"
     ]
    }
   ],
   "source": [
    "# compile\n",
    "\n",
    "\n",
    "from dspy.teleprompt import BootstrapFewShot\n",
    "\n",
    "# Validation logic: check that the predicted answer is correct.\n",
    "# Also check that the retrieved context does actually contain that answer.\n",
    "def validate_context_and_answer(example, pred, trace=None):\n",
    "    answer_EM = dspy.evaluate.answer_exact_match(example, pred)\n",
    "    answer_PM = dspy.evaluate.answer_passage_match(example, pred)\n",
    "    return answer_EM and answer_PM\n",
    "\n",
    "# Set up a basic teleprompter, which will compile our RAG program.\n",
    "teleprompter = BootstrapFewShot(metric=validate_context_and_answer)\n",
    "\n",
    "# Compile!\n",
    "compiled_rag = teleprompter.compile(RAG(), trainset=dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = './v1.json'\n",
    "compiled_rag.save(path = save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m2024-06-02T00:12:26.264912Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=120)\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m147\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_74f26 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_74f26 td {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_74f26_row0_col0, #T_74f26_row0_col1, #T_74f26_row0_col2, #T_74f26_row0_col3, #T_74f26_row0_col4, #T_74f26_row0_col5, #T_74f26_row1_col0, #T_74f26_row1_col1, #T_74f26_row1_col2, #T_74f26_row1_col3, #T_74f26_row1_col4, #T_74f26_row1_col5, #T_74f26_row2_col0, #T_74f26_row2_col1, #T_74f26_row2_col2, #T_74f26_row2_col3, #T_74f26_row2_col4, #T_74f26_row2_col5, #T_74f26_row3_col0, #T_74f26_row3_col1, #T_74f26_row3_col2, #T_74f26_row3_col3, #T_74f26_row3_col4, #T_74f26_row3_col5, #T_74f26_row4_col0, #T_74f26_row4_col1, #T_74f26_row4_col2, #T_74f26_row4_col3, #T_74f26_row4_col4, #T_74f26_row4_col5 {\n",
       "  text-align: left;\n",
       "  white-space: pre-wrap;\n",
       "  word-wrap: break-word;\n",
       "  max-width: 400px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_74f26\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_74f26_level0_col0\" class=\"col_heading level0 col0\" >question</th>\n",
       "      <th id=\"T_74f26_level0_col1\" class=\"col_heading level0 col1\" >example_answer</th>\n",
       "      <th id=\"T_74f26_level0_col2\" class=\"col_heading level0 col2\" >context</th>\n",
       "      <th id=\"T_74f26_level0_col3\" class=\"col_heading level0 col3\" >pred_answer</th>\n",
       "      <th id=\"T_74f26_level0_col4\" class=\"col_heading level0 col4\" >answer_exact_match</th>\n",
       "      <th id=\"T_74f26_level0_col5\" class=\"col_heading level0 col5\" >answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_74f26_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_74f26_row0_col0\" class=\"data row0 col0\" >In what ways has the Claude model's ability to self-critique and revise its responses enhanced its transparency?</td>\n",
       "      <td id=\"T_74f26_row0_col1\" class=\"data row0 col1\" >The Claude model's ability to self-critique and revise its responses has enhanced its transparency by allowing for iterative improvements based on past actions and mistakes....</td>\n",
       "      <td id=\"T_74f26_row0_col2\" class=\"data row0 col2\" >['could be a divergence between what a user actually intended and what the labeler thought was\\nintended from only reading the prompt.\\nIt is unclear how to...</td>\n",
       "      <td id=\"T_74f26_row0_col3\" class=\"data row0 col3\" >By iteratively revising based on previous outputs and user feedback.</td>\n",
       "      <td id=\"T_74f26_row0_col4\" class=\"data row0 col4\" >False</td>\n",
       "      <td id=\"T_74f26_row0_col5\" class=\"data row0 col5\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_74f26_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_74f26_row1_col0\" class=\"data row1 col0\" >How do subsequent versions of Claude compare in terms of their likelihood to produce false statements?</td>\n",
       "      <td id=\"T_74f26_row1_col1\" class=\"data row1 col1\" >Claude Instant is a faster and lighter version of Claude, with an input context length of 100,000 tokens. In contrast, Claude 3 has faced criticism...</td>\n",
       "      <td id=\"T_74f26_row1_col2\" class=\"data row1 col2\" >['for tasks having queries that may be factually incor-\\nrect (e.g., fact verification), spans of high lexical\\noverlap to an erroneous claim may reinforce the\\nmisinformation and lead...</td>\n",
       "      <td id=\"T_74f26_row1_col3\" class=\"data row1 col3\" >Information insufficient for comparison</td>\n",
       "      <td id=\"T_74f26_row1_col4\" class=\"data row1 col4\" >False</td>\n",
       "      <td id=\"T_74f26_row1_col5\" class=\"data row1 col5\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_74f26_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_74f26_row2_col0\" class=\"data row2 col0\" >Who developed the language model family known as Chinchilla?</td>\n",
       "      <td id=\"T_74f26_row2_col1\" class=\"data row2 col1\" >The Chinchilla language model family was developed by the research team at DeepMind and presented in March 2022. It is named 'Chinchilla' as an advancement...</td>\n",
       "      <td id=\"T_74f26_row2_col2\" class=\"data row2 col2\" >['Zhaojiang Lin, Andrea Madotto, and Pascale Fung. Exploring versatile generative language model\\nvia parameter-efﬁcient transfer learning. In Findings of the Association for Computational Lin-\\nguistics: EMNLP 2020,...</td>\n",
       "      <td id=\"T_74f26_row2_col3\" class=\"data row2 col3\" >Microsoft developed Chinchilla's language model family</td>\n",
       "      <td id=\"T_74f26_row2_col4\" class=\"data row2 col4\" >False</td>\n",
       "      <td id=\"T_74f26_row2_col5\" class=\"data row2 col5\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_74f26_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_74f26_row3_col0\" class=\"data row3 col0\" >What benchmark did Chinchilla achieve an average accuracy of 67.5% on?</td>\n",
       "      <td id=\"T_74f26_row3_col1\" class=\"data row3 col1\" >nan</td>\n",
       "      <td id=\"T_74f26_row3_col2\" class=\"data row3 col2\" >nan</td>\n",
       "      <td id=\"T_74f26_row3_col3\" class=\"data row3 col3\" >nan</td>\n",
       "      <td id=\"T_74f26_row3_col4\" class=\"data row3 col4\" >0.0</td>\n",
       "      <td id=\"T_74f26_row3_col5\" class=\"data row3 col5\" >Chinchilla achieved an average accuracy of 67.5% on the MMLU benchmark (Measuring Massive Multitask Language Understanding).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_74f26_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_74f26_row4_col0\" class=\"data row4 col0\" >What is the relationship between Chinchilla and the Gopher language model families?</td>\n",
       "      <td id=\"T_74f26_row4_col1\" class=\"data row4 col1\" >The Chinchilla family of transformer models is essentially the same as the Gopher family, with minor modifications and different training optimizers. Chinchilla uses AdamW optimizer...</td>\n",
       "      <td id=\"T_74f26_row4_col2\" class=\"data row4 col2\" >['language models from human preferences. arXiv preprint\\narXiv:1909.08593, 2019.\\n12', 'We evaluate the 7B and 33B LLaMA model variants that are extended via Position Interpolation or\\ndirect fine-tuning....</td>\n",
       "      <td id=\"T_74f26_row4_col3\" class=\"data row4 col3\" >Context: may or might not contain relevant facts or answer keywords Question: What is the relationship between Chinchilla and the Gopher language model families? Reasoning:...</td>\n",
       "      <td id=\"T_74f26_row4_col4\" class=\"data row4 col4\" >False</td>\n",
       "      <td id=\"T_74f26_row4_col5\" class=\"data row4 col5\" >nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x32f8f71c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dspy.evaluate.evaluate import Evaluate\n",
    "\n",
    "# Set up the `evaluate_on_hotpotqa` function. We'll use this many times below.\n",
    "evaluate_on_hotpotqa = Evaluate(devset=dataset_eval, num_threads=1, display_progress=False, display_table=5)\n",
    "\n",
    "# Evaluate the `compiled_rag` program with the `answer_exact_match` metric.\n",
    "metric = dspy.evaluate.answer_exact_match\n",
    "evaluate_on_hotpotqa(compiled_rag, metric=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m2024-06-02T00:14:15.376192Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t 'gold_titles'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m147\u001b[0m\n",
      "\u001b[2m2024-06-02T00:14:37.970705Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t 'gold_titles'\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m147\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m     found_titles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mmap\u001b[39m(dspy\u001b[38;5;241m.\u001b[39mevaluate\u001b[38;5;241m.\u001b[39mnormalize_text, [c\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m pred\u001b[38;5;241m.\u001b[39mcontext]))\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gold_titles\u001b[38;5;241m.\u001b[39missubset(found_titles)\n\u001b[0;32m----> 7\u001b[0m compiled_rag_retrieval_score \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_on_hotpotqa\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompiled_rag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgold_passages_retrieved\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/dspy/evaluate/evaluate.py:158\u001b[0m, in \u001b[0;36mEvaluate.__call__\u001b[0;34m(self, program, metric, devset, num_threads, display_progress, display_table, return_all_scores, return_outputs)\u001b[0m\n\u001b[1;32m    155\u001b[0m tqdm\u001b[38;5;241m.\u001b[39mtqdm\u001b[38;5;241m.\u001b[39m_instances\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_threads \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 158\u001b[0m     reordered_devset, ncorrect, ntotal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_single_thread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapped_program\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisplay_progress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    160\u001b[0m     reordered_devset, ncorrect, ntotal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_multi_thread(\n\u001b[1;32m    161\u001b[0m         wrapped_program,\n\u001b[1;32m    162\u001b[0m         devset,\n\u001b[1;32m    163\u001b[0m         num_threads,\n\u001b[1;32m    164\u001b[0m         display_progress,\n\u001b[1;32m    165\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/dspy/evaluate/evaluate.py:67\u001b[0m, in \u001b[0;36mEvaluate._execute_single_thread\u001b[0;34m(self, wrapped_program, devset, display_progress)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, arg \u001b[38;5;129;01min\u001b[39;00m devset:\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m logging_redirect_tqdm():\n\u001b[0;32m---> 67\u001b[0m         example_idx, example, prediction, score \u001b[38;5;241m=\u001b[39m \u001b[43mwrapped_program\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m         reordered_devset\u001b[38;5;241m.\u001b[39mappend((example_idx, example, prediction, score))\n\u001b[1;32m     69\u001b[0m         ncorrect \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m score\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/dspy/evaluate/evaluate.py:127\u001b[0m, in \u001b[0;36mEvaluate.__call__.<locals>.wrapped_program\u001b[0;34m(example_idx, example)\u001b[0m\n\u001b[1;32m    124\u001b[0m     thread_stacks[threading\u001b[38;5;241m.\u001b[39mget_ident()] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(dspy\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mmain_stack)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 127\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[43mprogram\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mexample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     score \u001b[38;5;241m=\u001b[39m metric(\n\u001b[1;32m    129\u001b[0m         example,\n\u001b[1;32m    130\u001b[0m         prediction,\n\u001b[1;32m    131\u001b[0m     )  \u001b[38;5;66;03m# FIXME: TODO: What's the right order? Maybe force name-based kwargs!\u001b[39;00m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# increment assert and suggest failures to program's attributes\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/dspy/primitives/program.py:26\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[42], line 10\u001b[0m, in \u001b[0;36mRAG.forward\u001b[0;34m(self, question)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, question):\n\u001b[1;32m      9\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve(question)\u001b[38;5;241m.\u001b[39mpassages\n\u001b[0;32m---> 10\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dspy\u001b[38;5;241m.\u001b[39mPrediction(context\u001b[38;5;241m=\u001b[39mcontext, answer\u001b[38;5;241m=\u001b[39mprediction\u001b[38;5;241m.\u001b[39manswer)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/dspy/predict/predict.py:61\u001b[0m, in \u001b[0;36mPredict.__call__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/dspy/predict/chain_of_thought.py:59\u001b[0m, in \u001b[0;36mChainOfThought.forward\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     signature \u001b[38;5;241m=\u001b[39m new_signature\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m# template = dsp.Template(self.signature.instructions, **new_signature)\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/dspy/predict/predict.py:103\u001b[0m, in \u001b[0;36mPredict.forward\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m template \u001b[38;5;241m=\u001b[39m signature_to_template(signature)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 103\u001b[0m     x, C \u001b[38;5;241m=\u001b[39m \u001b[43mdsp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# Note: query_only=True means the instructions and examples are not included.\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# I'm not really sure why we'd want to do that, but it's there.\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dsp\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mcontext(lm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm, query_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/dsp/primitives/predict.py:77\u001b[0m, in \u001b[0;36m_generate.<locals>.do_generate\u001b[0;34m(example, stage, max_depth, original_example)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Generate and extract the fields.\u001b[39;00m\n\u001b[1;32m     76\u001b[0m prompt \u001b[38;5;241m=\u001b[39m template(example)\n\u001b[0;32m---> 77\u001b[0m completions: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]] \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m completions: \u001b[38;5;28mlist\u001b[39m[Example] \u001b[38;5;241m=\u001b[39m [template\u001b[38;5;241m.\u001b[39mextract(example, p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m completions]\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Find the completions that are most complete.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/dsp/modules/ollama.py:171\u001b[0m, in \u001b[0;36mOllamaLocal.__call__\u001b[0;34m(self, prompt, only_completed, return_sorted, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m only_completed, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor now\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m return_sorted \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor now\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 171\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m choices \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    175\u001b[0m completed_choices \u001b[38;5;241m=\u001b[39m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m choices \u001b[38;5;28;01mif\u001b[39;00m c[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinish_reason\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/dsp/modules/ollama.py:145\u001b[0m, in \u001b[0;36mOllamaLocal.request\u001b[0;34m(self, prompt, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbasic_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/dsp/modules/ollama.py:97\u001b[0m, in \u001b[0;36mOllamaLocal.basic_request\u001b[0;34m(self, prompt, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m tot_eval_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[0;32m---> 97\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43murlstr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msettings_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout_s\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;66;03m# Check if the request was successful (HTTP status code 200)\u001b[39;00m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;66;03m# If the request was not successful, print an error message\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    444\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    445\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m             \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/http/client.py:1377\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1377\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1378\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1379\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/http/client.py:320\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    322\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/http/client.py:281\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 281\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def gold_passages_retrieved(example, pred, trace=None):\n",
    "    gold_titles = set(map(dspy.evaluate.normalize_text, example['gold_titles']))\n",
    "    found_titles = set(map(dspy.evaluate.normalize_text, [c.split(' | ')[0] for c in pred.context]))\n",
    "\n",
    "    return gold_titles.issubset(found_titles)\n",
    "\n",
    "compiled_rag_retrieval_score = evaluate_on_hotpotqa(compiled_rag, metric=gold_passages_retrieved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 passages for question: What benchmarks or ratings are used to compare the capabilities of different language models? \n",
      " ------------------------------ \n",
      "\n",
      "1] 34.5\n",
      "41.6\n",
      "53.6\n",
      "59.8\n",
      "HH-RLHF\n",
      "34.9\n",
      "44.6\n",
      "55.8\n",
      "60.1\n",
      "Unnatural Instruct\n",
      "41.9\n",
      "48.1\n",
      "57.3\n",
      "61.3\n",
      "Guanaco (OASST1)\n",
      "36.6\n",
      "46.4\n",
      "57.0\n",
      "62.2\n",
      "Alpaca\n",
      "38.8\n",
      "47.8\n",
      "57.3\n",
      "62.5\n",
      "FLAN v2\n",
      "44.5\n",
      "51.4\n",
      "59.2\n",
      "63.9\n",
      "Following common practice, we use the MMLU (Mas-\n",
      "sively Multitask Language Understanding) benchmark\n",
      "[24] to measure performance on a range of language un-\n",
      "derstanding tasks. This is a multiple-choice benchmark\n",
      "covering 57 tasks including elementary mathematics,\n",
      "US history, computer science, law, and more. We report\n",
      "5-shot test accuracy.\n",
      "We also test generative language capabilities through\n",
      "both automated and human evaluations. This second\n",
      "set of evaluations relies on queries curated by humans\n",
      "and aims at measuring the quality of model responses.\n",
      "While this is a more realistic testbed for chatbot model \n",
      "\n",
      "2] may negatively affect the language model’s performance. We present more benchmark results on\n",
      "the original context window size in Section 3.4.\n",
      "In Table 3 we report the relationship between perplexity and the number of fine-tuning steps for\n",
      "LLaMA 7B model extending to 8192 and 16384 context window sizes using Position Interpolation\n",
      "evaluated on the PG19 dataset. We can see without fine-tuning (at step 0) the model can exhibit\n",
      "certain language modeling capability, as indicated by < 20 perplexity for extending to 8192 context\n",
      "window (in contrast, the direct extrapolation method leads to > 103 perplexity). With fine-tuning,\n",
      "we observed that the perplexity improves quickly. At 200 steps the models surpassed the original \n",
      "\n",
      "3] pick the best response or declare a tie and provide an explanation. We conduct these head-to-head\n",
      "comparisons on all permutations of pairs of systems on both the Vicuna and OA benchmarks.\n",
      "Human Evaluation\n",
      "While recent work indicates generative models can be effectively employed\n",
      "for system evaluations [19], the reliability GPT-4 ratings to assess chatbot performance is, to our\n",
      "knowledge, yet to be proven to correlate with human judgments. Therefore, we run two parallel\n",
      "human evaluations on the Vicuna benchmark matching both automated evaluation protocols described\n",
      "above. We use Amazon Mechanical Turk (AMT) and get two human annotators for comparisons to\n",
      "ChatGPT and three annotators for pairwise comparisons.\n",
      "Elo Rating \n",
      "\n",
      "Question: What benchmarks or ratings are used to compare the capabilities of different language models?\n",
      "Predicted Answer: Benchmarks like MMLU, perplexity tests with varying context windows and the PG19 dataset; plus ratings from Vicuna/OA benchmarks and AMT-based human evaluations compare different language model capabilities.\n",
      "Retrieved Contexts (truncated): ['34.5\\n41.6\\n53.6\\n59.8\\nHH-RLHF\\n34.9\\n44.6\\n55.8\\n60.1\\nUnnatural Instruct\\n41.9\\n48.1\\n57.3\\n61.3\\nGuanaco (OASST1)\\n36.6\\n46.4\\n57.0\\n62.2\\nAlpaca\\n38.8\\n47.8\\n57.3\\n62.5\\nFLAN v2\\n44.5\\n51.4\\n59.2\\n63.9\\nFollowing common prac...', 'may negatively affect the language model’s performance. We present more benchmark results on\\nthe original context window size in Section 3.4.\\nIn Table 3 we report the relationship between perplexity a...', 'pick the best response or declare a tie and provide an explanation. We conduct these head-to-head\\ncomparisons on all permutations of pairs of systems on both the Vicuna and OA benchmarks.\\nHuman Evalua...']\n"
     ]
    }
   ],
   "source": [
    "# Ask any question you like to this simple RAG program.\n",
    "\n",
    "# my_question = \"What are some of the evaluation metrics used for assessing different types of text generation tasks presented in the study?\"\n",
    "# my_question = \"What are the key advantages of InstructGPT models over GPT-3 models according to the findings in the research?\"\n",
    "# my_question = \"When was the transformer architecture introduced, and by which organization?\" \n",
    "# my_question =  \"How has the accessibility of powerful language models, such as GPT-3 and GPT-4, been controlled by their developers?\"\n",
    "my_question =  \"What benchmarks or ratings are used to compare the capabilities of different language models?\"\n",
    "# my_question =  \"What are some of the primary applications for language models in technology and computing?\"\n",
    "# my_question =  \"How are language models typically evaluated and what benchmarks are used for this purpose?\"\n",
    "# my_question =  \"What datasets are available for evaluating language processing systems?\"\n",
    "# my_question = \"What collaborations with other companies have contributed to the development of Claude's capabilities?\"\n",
    "# my_question = \"According to DeepMind, how should the number of training tokens change relative to the model size?\"\n",
    "# my_question =  \"How do the sizes of models in the Gopher family range?\"\n",
    "# 31: {\"question\": \"What type of model architecture do the Gopher and Chinchilla families belong to?\"},\n",
    "# 32: {\"question\": \"Can you name the author who wrote the novels A Farewell to Arms and The Sun Also Rises?\"},\n",
    "# 37: {\"question\": \"What are the key advantages of InstructGPT models over GPT-3 models according to the findings in the research?\"},\n",
    "# 40: {\"question\": \"What metrics are used to compare the performance of different models on training and validation splits according to the document provided?\"},\n",
    "# 42: {\"question\": \"What types of evaluation metrics are commonly used to assess the accuracy of answers in AI-driven question and answer datasets?\"},\n",
    "# 49: {\"question\": \"What factors contribute to the performance improvement in retrieval-augmented language models compared to non-retrieval-augmented models?\"},\n",
    "# 56: {\"question\": \"What are the benchmarks used to evaluate the performance of the Deep Policy Optimization (DPO) method compared to other preference learning algorithms in the document provided?\"},\n",
    "# 57: {\"question\": \"What methodologies have been evaluated for training language models to align with human preferences, and how do they compare in terms of effectiveness?\"},\n",
    "# 58: {\"question\": \"What methods have been discussed in the literature for improving the alignment of language models with human preferences or feedback?\"},\n",
    "# 66: {\"question\": \"What are some of the evaluation metrics used for assessing different types of text generation tasks presented in the study?\"},\n",
    "# 68: {\"question\": \"Consider a document related to research in natural language processing or artificial intelligence. Can you name some of the recent topics or methods that have been discussed or introduced in the field according to the document?\"},\n",
    "# 71: {\"question\": \"What is the significance of using reflection tokens in a model like SELF-RAG?\"},\n",
    "# 72: {\"question\": \"How does the inclusion of selected context as opposed to appending all retrieved text spans impact computational cost during both training and inference times in language model generation tasks?\"},\n",
    "# 77: {\"question\": \"What are the benefits of modeling human biases in Human-Aware Loss Optimizations (HALOs), and how do they compare to non-HALOs on the same datasets?\"},\n",
    "# 79: {\"question\": \"What are the modifications made to the traditional Kahneman-Tversky model to adapt it for optimizing language model performance?\"},\n",
    "# 83: {\"question\": \"How does a model's ability to answer questions relate to its exposure to specific types of questions during training?\"},\n",
    "# 90: {\"question\": \"How can adding examples to a prompt affect the performance of language models?\"},\n",
    "# 98: {\"question\": \"What are the main components of a Neural Turing Machine (NTM) architecture?\"},\n",
    "# 99: {\"question\": \"How might a seq2seq model's limitations be addressed in natural language processing tasks?\"},\n",
    "# 100: {\"question\": \"What differentiates hard attention from soft attention in image processing algorithms?\"},\n",
    "\n",
    "get_top_passages(my_question)\n",
    "\n",
    "# Get the prediction. This contains `pred.context` and `pred.answer`.\n",
    "pred = compiled_rag(my_question)\n",
    "\n",
    "# Print the contexts and the answer.\n",
    "print(f\"Question: {my_question}\")\n",
    "print(f\"Predicted Answer: {pred.answer}\")\n",
    "print(f\"Retrieved Contexts (truncated): {[c[:200] + '...' for c in pred.context]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Answer questions with short factoid answers.\n",
      "\n",
      "---\n",
      "\n",
      "Question: How have language models evolved in terms of architecture, from the 1980s to present times?\n",
      "Answer: Language models have evolved significantly in terms of architecture from the 1980s to present times. In the 1980s, the first statistical language model was proposed, leading to experiments by IBM that identified areas for improvement by observing human subjects. However, it wasn't until 2017 when the transformer architecture was introduced by Google, revolutionizing the field. This development paved the way for models like BERT in 2018, which marked a shift towards large-scale transformer-based language models. These modern architectures, based on self-attention mechanisms, have dominated the field of natural language processing, achieving state-of-the-art performance in various tasks.\n",
      "\n",
      "Question: Can you name some specific large language models and the companies or organizations that have developed them?\n",
      "Answer: Some specific large language models include GPT-3 by OpenAI, Chinchilla by DeepMind, and BERT by Google. OpenAI developed GPT-3, DeepMind developed Chinchilla, and Google developed BERT. These models have been significant advancements in the field of natural language processing.\n",
      "\n",
      "Question: How do advances in AI models impact their ability to interact with different types of data, such as images?\n",
      "Answer: Advances in AI models, such as multimodal models like RA-CM3, have significantly improved their ability to interact with different types of data, such as images. These models can refer to external memory, like web data, to increase their knowledge capacity, allowing them to generate correct images from entity-rich captions. Additionally, these models can perform image editing and manually specify examples in-context for better results. The use of large language models, combined with larger datasets and neural networks, has also enhanced their performance in tasks like image generation and text generation.\n",
      "\n",
      "Question: What are the potential trade-offs between AI system alignment with ethical guidelines and practical utility?\n",
      "Answer: The potential trade-offs between AI system alignment with ethical guidelines and practical utility include the risk of reduced performance and usability due to stringent ethical alignment measures, as seen with Claude 2. Users may face limitations and refusal of assistance for benign requests, leading to debates over the 'alignment tax' in AI development. Balancing ethical considerations with practical functionality is crucial to ensure alignment with ethical guidelines without compromising the practical utility of AI systems. Research is needed to find a middle ground that prioritizes ethical alignment while maintaining usability and performance.\n",
      "\n",
      "Question: How does a large language model learn from text during training?\n",
      "Answer: A large language model learns from text during training by first going through an unsupervised generative 'pretraining' stage where it sets initial parameters using a language modeling objective. Then, it goes through a supervised discriminative 'fine-tuning' stage where it refines its parameters based on annotated examples or task demonstrations. This dual-stage approach allows the model to learn statistical relationships from text documents in a computationally intensive process, enabling it to achieve general-purpose language generation and natural language processing tasks.\n",
      "\n",
      "Question: What is Constitutional AI and how does it affect the functionality of AI systems?\n",
      "Answer: Constitutional AI is an approach developed by Anthropic for training AI systems, particularly language models like Claude, to be harmless and helpful without relying on extensive human feedback. It involves two phases: supervised learning, where the model generates responses to prompts and self-critiques based on a set of guiding principles, and reinforcement learning, where the model is trained with AI-generated feedback according to constitutional principles. This approach enables the training of AI assistants that are both helpful and harmless, with the ability to explain objections to harmful requests, enhancing transparency and reducing the need for human supervision.\n",
      "\n",
      "Question: What is the benefit of using continuous space embeddings in recurrent neural network language models?\n",
      "Answer: Continuous space embeddings in recurrent neural network language models help alleviate the curse of dimensionality by representing words as non-linear combinations of weights in the embedding space. This approach helps address the data sparsity problem caused by the exponential increase in possible word sequences with vocabulary size. By utilizing continuous space embeddings, neural networks can effectively capture semantic relationships and meaning within the language model.\n",
      "\n",
      "Question: Can you explain how maximum entropy language models work and what the partition function signifies?\n",
      "Answer: Maximum entropy language models use feature functions to encode the relationship between a word and its n-gram history, aiming to maximize reward while satisfying a KL-constrained objective. The partition function, denoted as Z(x), is crucial in normalizing the probabilities of all possible outputs given the input. It represents the sum of the exponential of the reward function over all possible output sequences, making it computationally expensive to estimate but essential for accurate modeling. The partition function ensures that the model's predicted probabilities sum up to 1, providing a foundation for effective language modeling.\n",
      "\n",
      "Question: What are language models and what is their purpose in natural language processing?\n",
      "Answer: Language models are probabilistic models of natural language that help predict or correct text. Their purpose in natural language processing is to assist in various tasks such as speech recognition, machine translation, natural language generation, and information retrieval. By analyzing the performance of human subjects, language models improve the understanding and generation of human-like text.\n",
      "\n",
      "Question: What challenges do large language models face in mirroring human cognitive patterns?\n",
      "Answer: Large language models face challenges in mirroring human cognitive patterns because they sometimes learn patterns that humans do not learn, while also failing to learn patterns that humans typically learn. This discrepancy suggests that the models may not be plausible cognitive models, despite matching human performance in some tasks. Further research is needed to address these limitations and improve the alignment of large language models with human cognitive patterns.\n",
      "\n",
      "Question: How has the token handling capacity changed between different versions of the Claude model?\n",
      "Answer: The token handling capacity has increased with each new version of the Claude model. Claude Instant has a context length of 100,000 tokens, Claude 2.1 doubled this to 200,000 tokens, and Claude 3 Opus default version has a context window of 200,000 tokens but can be expanded to 1 million for specific use cases. This progression shows a trend towards handling larger amounts of text data for improved performance and capabilities.\n",
      "\n",
      "Question: What are some key architectures behind the development of large language models?\n",
      "Answer: Key architectures behind the development of large language models include the use of self-attention mechanisms, such as those seen in Transformer decoders. These architectures have been applied to tasks like autoregressive language modeling and have led to the dominance of Transformer-based language models in NLP. Models like BERT and GPT-2 have further advanced this paradigm, showcasing the power of large Transformer language models in achieving state-of-the-art results across various NLP tasks. Additionally, architectures like neural-retriever-in-the-loop generative-based models have shown improvements in tasks like open-domain QA and knowledge-grounded dialogue, emphasizing the importance of consistent and engaging responses in long-form generation and multi-turn conversations.\n",
      "\n",
      "Question: What licensing models have been adopted for the distribution of source-available language models?\n",
      "Answer: Based on the provided context, it seems that licensing models for the distribution of source-available language models have not been explicitly discussed in the referenced papers. However, it is crucial to consider potential licensing options such as open-source licenses (e.g., GPL, MIT) or proprietary licenses when distributing language models to ensure legal compliance and control over usage rights. Additionally, considering the implications of different licensing models on accessibility, collaboration, and commercialization is essential for determining the most suitable approach for sharing language models with the community. Further research or consultation with legal experts may be necessary to explore specific licensing strategies for source-available language models.\n",
      "\n",
      "Question: What purpose do large language models serve in the field of natural language processing?\n",
      "Answer: Large language models (LLMs) serve the purpose of enabling general-purpose language generation and other natural language processing tasks such as classification. They achieve this by learning statistical relationships from text documents during computationally intensive self-supervised and semi-supervised training. LLMs can be used for text generation by predicting the next token or word, making them valuable for tasks like speech recognition, machine translation, and information retrieval. Additionally, LLMs have superseded previous models like recurrent neural networks, showcasing their efficiency and effectiveness in NLP tasks.\n",
      "\n",
      "Question: What factors influenced the development of generative language models by Anthropic?\n",
      "Answer: Several factors influenced the development of generative language models by Anthropic, including the limitations in coding, math, and reasoning capabilities of the initial version Claude, the partnerships with companies like Notion and Quora to enhance the model's capabilities, and the need to address biases, unsafe content, and ethical considerations in training data. Additionally, the reliance on supervised learning and the need for controlled generation in generative models played a role in shaping the development of Anthropic's language models.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context: may or maynot contain relevant facts or answer keywords\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: an answer between 10 to 20 words\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «34.5\n",
      "41.6\n",
      "53.6\n",
      "59.8\n",
      "HH-RLHF\n",
      "34.9\n",
      "44.6\n",
      "55.8\n",
      "60.1\n",
      "Unnatural Instruct\n",
      "41.9\n",
      "48.1\n",
      "57.3\n",
      "61.3\n",
      "Guanaco (OASST1)\n",
      "36.6\n",
      "46.4\n",
      "57.0\n",
      "62.2\n",
      "Alpaca\n",
      "38.8\n",
      "47.8\n",
      "57.3\n",
      "62.5\n",
      "FLAN v2\n",
      "44.5\n",
      "51.4\n",
      "59.2\n",
      "63.9\n",
      "Following common practice, we use the MMLU (Mas-\n",
      "sively Multitask Language Understanding) benchmark\n",
      "[24] to measure performance on a range of language un-\n",
      "derstanding tasks. This is a multiple-choice benchmark\n",
      "covering 57 tasks including elementary mathematics,\n",
      "US history, computer science, law, and more. We report\n",
      "5-shot test accuracy.\n",
      "We also test generative language capabilities through\n",
      "both automated and human evaluations. This second\n",
      "set of evaluations relies on queries curated by humans\n",
      "and aims at measuring the quality of model responses.\n",
      "While this is a more realistic testbed for chatbot model»\n",
      "[2] «may negatively affect the language model’s performance. We present more benchmark results on\n",
      "the original context window size in Section 3.4.\n",
      "In Table 3 we report the relationship between perplexity and the number of fine-tuning steps for\n",
      "LLaMA 7B model extending to 8192 and 16384 context window sizes using Position Interpolation\n",
      "evaluated on the PG19 dataset. We can see without fine-tuning (at step 0) the model can exhibit\n",
      "certain language modeling capability, as indicated by < 20 perplexity for extending to 8192 context\n",
      "window (in contrast, the direct extrapolation method leads to > 103 perplexity). With fine-tuning,\n",
      "we observed that the perplexity improves quickly. At 200 steps the models surpassed the original»\n",
      "[3] «pick the best response or declare a tie and provide an explanation. We conduct these head-to-head\n",
      "comparisons on all permutations of pairs of systems on both the Vicuna and OA benchmarks.\n",
      "Human Evaluation\n",
      "While recent work indicates generative models can be effectively employed\n",
      "for system evaluations [19], the reliability GPT-4 ratings to assess chatbot performance is, to our\n",
      "knowledge, yet to be proven to correlate with human judgments. Therefore, we run two parallel\n",
      "human evaluations on the Vicuna benchmark matching both automated evaluation protocols described\n",
      "above. We use Amazon Mechanical Turk (AMT) and get two human annotators for comparisons to\n",
      "ChatGPT and three annotators for pairwise comparisons.\n",
      "Elo Rating»\n",
      "\n",
      "Question: What benchmarks or ratings are used to compare the capabilities of different language models?\n",
      "\n",
      "Reasoning: Let's think step by step in order to Answer: MMLU, perplexity on context window sizes and PG19 dataset, Vicuna and OA benchmarks, human evaluations using Amazon Mechanical Turk (AMT) are used for comparing language models.\n",
      "\n",
      "Answer:\u001b[32m Benchmarks like MMLU, perplexity tests with varying context windows and the PG19 dataset; plus ratings from Vicuna/OA benchmarks and AMT-based human evaluations compare different language model capabilities.\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\nAnswer questions with short factoid answers.\\n\\n---\\n\\nQuestion: How have language models evolved in terms of architecture, from the 1980s to present times?\\nAnswer: Language models have evolved significantly in terms of architecture from the 1980s to present times. In the 1980s, the first statistical language model was proposed, leading to experiments by IBM that identified areas for improvement by observing human subjects. However, it wasn't until 2017 when the transformer architecture was introduced by Google, revolutionizing the field. This development paved the way for models like BERT in 2018, which marked a shift towards large-scale transformer-based language models. These modern architectures, based on self-attention mechanisms, have dominated the field of natural language processing, achieving state-of-the-art performance in various tasks.\\n\\nQuestion: Can you name some specific large language models and the companies or organizations that have developed them?\\nAnswer: Some specific large language models include GPT-3 by OpenAI, Chinchilla by DeepMind, and BERT by Google. OpenAI developed GPT-3, DeepMind developed Chinchilla, and Google developed BERT. These models have been significant advancements in the field of natural language processing.\\n\\nQuestion: How do advances in AI models impact their ability to interact with different types of data, such as images?\\nAnswer: Advances in AI models, such as multimodal models like RA-CM3, have significantly improved their ability to interact with different types of data, such as images. These models can refer to external memory, like web data, to increase their knowledge capacity, allowing them to generate correct images from entity-rich captions. Additionally, these models can perform image editing and manually specify examples in-context for better results. The use of large language models, combined with larger datasets and neural networks, has also enhanced their performance in tasks like image generation and text generation.\\n\\nQuestion: What are the potential trade-offs between AI system alignment with ethical guidelines and practical utility?\\nAnswer: The potential trade-offs between AI system alignment with ethical guidelines and practical utility include the risk of reduced performance and usability due to stringent ethical alignment measures, as seen with Claude 2. Users may face limitations and refusal of assistance for benign requests, leading to debates over the 'alignment tax' in AI development. Balancing ethical considerations with practical functionality is crucial to ensure alignment with ethical guidelines without compromising the practical utility of AI systems. Research is needed to find a middle ground that prioritizes ethical alignment while maintaining usability and performance.\\n\\nQuestion: How does a large language model learn from text during training?\\nAnswer: A large language model learns from text during training by first going through an unsupervised generative 'pretraining' stage where it sets initial parameters using a language modeling objective. Then, it goes through a supervised discriminative 'fine-tuning' stage where it refines its parameters based on annotated examples or task demonstrations. This dual-stage approach allows the model to learn statistical relationships from text documents in a computationally intensive process, enabling it to achieve general-purpose language generation and natural language processing tasks.\\n\\nQuestion: What is Constitutional AI and how does it affect the functionality of AI systems?\\nAnswer: Constitutional AI is an approach developed by Anthropic for training AI systems, particularly language models like Claude, to be harmless and helpful without relying on extensive human feedback. It involves two phases: supervised learning, where the model generates responses to prompts and self-critiques based on a set of guiding principles, and reinforcement learning, where the model is trained with AI-generated feedback according to constitutional principles. This approach enables the training of AI assistants that are both helpful and harmless, with the ability to explain objections to harmful requests, enhancing transparency and reducing the need for human supervision.\\n\\nQuestion: What is the benefit of using continuous space embeddings in recurrent neural network language models?\\nAnswer: Continuous space embeddings in recurrent neural network language models help alleviate the curse of dimensionality by representing words as non-linear combinations of weights in the embedding space. This approach helps address the data sparsity problem caused by the exponential increase in possible word sequences with vocabulary size. By utilizing continuous space embeddings, neural networks can effectively capture semantic relationships and meaning within the language model.\\n\\nQuestion: Can you explain how maximum entropy language models work and what the partition function signifies?\\nAnswer: Maximum entropy language models use feature functions to encode the relationship between a word and its n-gram history, aiming to maximize reward while satisfying a KL-constrained objective. The partition function, denoted as Z(x), is crucial in normalizing the probabilities of all possible outputs given the input. It represents the sum of the exponential of the reward function over all possible output sequences, making it computationally expensive to estimate but essential for accurate modeling. The partition function ensures that the model's predicted probabilities sum up to 1, providing a foundation for effective language modeling.\\n\\nQuestion: What are language models and what is their purpose in natural language processing?\\nAnswer: Language models are probabilistic models of natural language that help predict or correct text. Their purpose in natural language processing is to assist in various tasks such as speech recognition, machine translation, natural language generation, and information retrieval. By analyzing the performance of human subjects, language models improve the understanding and generation of human-like text.\\n\\nQuestion: What challenges do large language models face in mirroring human cognitive patterns?\\nAnswer: Large language models face challenges in mirroring human cognitive patterns because they sometimes learn patterns that humans do not learn, while also failing to learn patterns that humans typically learn. This discrepancy suggests that the models may not be plausible cognitive models, despite matching human performance in some tasks. Further research is needed to address these limitations and improve the alignment of large language models with human cognitive patterns.\\n\\nQuestion: How has the token handling capacity changed between different versions of the Claude model?\\nAnswer: The token handling capacity has increased with each new version of the Claude model. Claude Instant has a context length of 100,000 tokens, Claude 2.1 doubled this to 200,000 tokens, and Claude 3 Opus default version has a context window of 200,000 tokens but can be expanded to 1 million for specific use cases. This progression shows a trend towards handling larger amounts of text data for improved performance and capabilities.\\n\\nQuestion: What are some key architectures behind the development of large language models?\\nAnswer: Key architectures behind the development of large language models include the use of self-attention mechanisms, such as those seen in Transformer decoders. These architectures have been applied to tasks like autoregressive language modeling and have led to the dominance of Transformer-based language models in NLP. Models like BERT and GPT-2 have further advanced this paradigm, showcasing the power of large Transformer language models in achieving state-of-the-art results across various NLP tasks. Additionally, architectures like neural-retriever-in-the-loop generative-based models have shown improvements in tasks like open-domain QA and knowledge-grounded dialogue, emphasizing the importance of consistent and engaging responses in long-form generation and multi-turn conversations.\\n\\nQuestion: What licensing models have been adopted for the distribution of source-available language models?\\nAnswer: Based on the provided context, it seems that licensing models for the distribution of source-available language models have not been explicitly discussed in the referenced papers. However, it is crucial to consider potential licensing options such as open-source licenses (e.g., GPL, MIT) or proprietary licenses when distributing language models to ensure legal compliance and control over usage rights. Additionally, considering the implications of different licensing models on accessibility, collaboration, and commercialization is essential for determining the most suitable approach for sharing language models with the community. Further research or consultation with legal experts may be necessary to explore specific licensing strategies for source-available language models.\\n\\nQuestion: What purpose do large language models serve in the field of natural language processing?\\nAnswer: Large language models (LLMs) serve the purpose of enabling general-purpose language generation and other natural language processing tasks such as classification. They achieve this by learning statistical relationships from text documents during computationally intensive self-supervised and semi-supervised training. LLMs can be used for text generation by predicting the next token or word, making them valuable for tasks like speech recognition, machine translation, and information retrieval. Additionally, LLMs have superseded previous models like recurrent neural networks, showcasing their efficiency and effectiveness in NLP tasks.\\n\\nQuestion: What factors influenced the development of generative language models by Anthropic?\\nAnswer: Several factors influenced the development of generative language models by Anthropic, including the limitations in coding, math, and reasoning capabilities of the initial version Claude, the partnerships with companies like Notion and Quora to enhance the model's capabilities, and the need to address biases, unsafe content, and ethical considerations in training data. Additionally, the reliance on supervised learning and the need for controlled generation in generative models played a role in shaping the development of Anthropic's language models.\\n\\n---\\n\\nFollow the following format.\\n\\nContext: may or maynot contain relevant facts or answer keywords\\n\\nQuestion: ${question}\\n\\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\\n\\nAnswer: an answer between 10 to 20 words\\n\\n---\\n\\nContext:\\n[1] «34.5\\n41.6\\n53.6\\n59.8\\nHH-RLHF\\n34.9\\n44.6\\n55.8\\n60.1\\nUnnatural Instruct\\n41.9\\n48.1\\n57.3\\n61.3\\nGuanaco (OASST1)\\n36.6\\n46.4\\n57.0\\n62.2\\nAlpaca\\n38.8\\n47.8\\n57.3\\n62.5\\nFLAN v2\\n44.5\\n51.4\\n59.2\\n63.9\\nFollowing common practice, we use the MMLU (Mas-\\nsively Multitask Language Understanding) benchmark\\n[24] to measure performance on a range of language un-\\nderstanding tasks. This is a multiple-choice benchmark\\ncovering 57 tasks including elementary mathematics,\\nUS history, computer science, law, and more. We report\\n5-shot test accuracy.\\nWe also test generative language capabilities through\\nboth automated and human evaluations. This second\\nset of evaluations relies on queries curated by humans\\nand aims at measuring the quality of model responses.\\nWhile this is a more realistic testbed for chatbot model»\\n[2] «may negatively affect the language model’s performance. We present more benchmark results on\\nthe original context window size in Section 3.4.\\nIn Table 3 we report the relationship between perplexity and the number of fine-tuning steps for\\nLLaMA 7B model extending to 8192 and 16384 context window sizes using Position Interpolation\\nevaluated on the PG19 dataset. We can see without fine-tuning (at step 0) the model can exhibit\\ncertain language modeling capability, as indicated by < 20 perplexity for extending to 8192 context\\nwindow (in contrast, the direct extrapolation method leads to > 103 perplexity). With fine-tuning,\\nwe observed that the perplexity improves quickly. At 200 steps the models surpassed the original»\\n[3] «pick the best response or declare a tie and provide an explanation. We conduct these head-to-head\\ncomparisons on all permutations of pairs of systems on both the Vicuna and OA benchmarks.\\nHuman Evaluation\\nWhile recent work indicates generative models can be effectively employed\\nfor system evaluations [19], the reliability GPT-4 ratings to assess chatbot performance is, to our\\nknowledge, yet to be proven to correlate with human judgments. Therefore, we run two parallel\\nhuman evaluations on the Vicuna benchmark matching both automated evaluation protocols described\\nabove. We use Amazon Mechanical Turk (AMT) and get two human annotators for comparisons to\\nChatGPT and three annotators for pairwise comparisons.\\nElo Rating»\\n\\nQuestion: What benchmarks or ratings are used to compare the capabilities of different language models?\\n\\nReasoning: Let's think step by step in order to Answer: MMLU, perplexity on context window sizes and PG19 dataset, Vicuna and OA benchmarks, human evaluations using Amazon Mechanical Turk (AMT) are used for comparing language models.\\n\\nAnswer:\\x1b[32m Benchmarks like MMLU, perplexity tests with varying context windows and the PG19 dataset; plus ratings from Vicuna/OA benchmarks and AMT-based human evaluations compare different language model capabilities.\\x1b[0m\\n\\n\\n\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama_model.inspect_history(n = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
